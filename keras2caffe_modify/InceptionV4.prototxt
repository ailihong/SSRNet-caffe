input: "data"
input_dim: 1
input_dim: 3
input_dim: 299
input_dim: 299
layer {
  name: "conv2d_1"
  type: "Convolution"
  bottom: "data"
  top: "conv2d_1"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_1"
  type: "BatchNorm"
  bottom: "conv2d_1"
  top: "conv2d_1"
}
layer {
  name: "batch_normalization_1s"
  type: "Scale"
  bottom: "conv2d_1"
  top: "conv2d_1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_1"
  type: "ReLU"
  bottom: "conv2d_1"
  top: "conv2d_1"
}
layer {
  name: "conv2d_2"
  type: "Convolution"
  bottom: "conv2d_1"
  top: "conv2d_2"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_2"
  type: "BatchNorm"
  bottom: "conv2d_2"
  top: "conv2d_2"
}
layer {
  name: "batch_normalization_2s"
  type: "Scale"
  bottom: "conv2d_2"
  top: "conv2d_2"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_2"
  type: "ReLU"
  bottom: "conv2d_2"
  top: "conv2d_2"
}
layer {
  name: "conv2d_3"
  type: "Convolution"
  bottom: "conv2d_2"
  top: "conv2d_3"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_3"
  type: "BatchNorm"
  bottom: "conv2d_3"
  top: "conv2d_3"
}
layer {
  name: "batch_normalization_3s"
  type: "Scale"
  bottom: "conv2d_3"
  top: "conv2d_3"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_3"
  type: "ReLU"
  bottom: "conv2d_3"
  top: "conv2d_3"
}
layer {
  name: "conv2d_4"
  type: "Convolution"
  bottom: "conv2d_3"
  top: "conv2d_4"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_4"
  type: "BatchNorm"
  bottom: "conv2d_4"
  top: "conv2d_4"
}
layer {
  name: "batch_normalization_4s"
  type: "Scale"
  bottom: "conv2d_4"
  top: "conv2d_4"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "max_pooling2d_1"
  type: "Pooling"
  bottom: "conv2d_3"
  top: "max_pooling2d_1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "activation_4"
  type: "ReLU"
  bottom: "conv2d_4"
  top: "conv2d_4"
}
layer {
  name: "concatenate_1"
  type: "Concat"
  bottom: "max_pooling2d_1"
  bottom: "conv2d_4"
  top: "concatenate_1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_7"
  type: "Convolution"
  bottom: "concatenate_1"
  top: "conv2d_7"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_7"
  type: "BatchNorm"
  bottom: "conv2d_7"
  top: "conv2d_7"
}
layer {
  name: "batch_normalization_7s"
  type: "Scale"
  bottom: "conv2d_7"
  top: "conv2d_7"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_7"
  type: "ReLU"
  bottom: "conv2d_7"
  top: "conv2d_7"
}
layer {
  name: "conv2d_8"
  type: "Convolution"
  bottom: "conv2d_7"
  top: "conv2d_8"
  convolution_param {
    num_output: 64
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_8"
  type: "BatchNorm"
  bottom: "conv2d_8"
  top: "conv2d_8"
}
layer {
  name: "batch_normalization_8s"
  type: "Scale"
  bottom: "conv2d_8"
  top: "conv2d_8"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_8"
  type: "ReLU"
  bottom: "conv2d_8"
  top: "conv2d_8"
}
layer {
  name: "conv2d_5"
  type: "Convolution"
  bottom: "concatenate_1"
  top: "conv2d_5"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_9"
  type: "Convolution"
  bottom: "conv2d_8"
  top: "conv2d_9"
  convolution_param {
    num_output: 64
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_5"
  type: "BatchNorm"
  bottom: "conv2d_5"
  top: "conv2d_5"
}
layer {
  name: "batch_normalization_5s"
  type: "Scale"
  bottom: "conv2d_5"
  top: "conv2d_5"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_9"
  type: "BatchNorm"
  bottom: "conv2d_9"
  top: "conv2d_9"
}
layer {
  name: "batch_normalization_9s"
  type: "Scale"
  bottom: "conv2d_9"
  top: "conv2d_9"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_5"
  type: "ReLU"
  bottom: "conv2d_5"
  top: "conv2d_5"
}
layer {
  name: "activation_9"
  type: "ReLU"
  bottom: "conv2d_9"
  top: "conv2d_9"
}
layer {
  name: "conv2d_6"
  type: "Convolution"
  bottom: "conv2d_5"
  top: "conv2d_6"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_10"
  type: "Convolution"
  bottom: "conv2d_9"
  top: "conv2d_10"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_6"
  type: "BatchNorm"
  bottom: "conv2d_6"
  top: "conv2d_6"
}
layer {
  name: "batch_normalization_6s"
  type: "Scale"
  bottom: "conv2d_6"
  top: "conv2d_6"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_10"
  type: "BatchNorm"
  bottom: "conv2d_10"
  top: "conv2d_10"
}
layer {
  name: "batch_normalization_10s"
  type: "Scale"
  bottom: "conv2d_10"
  top: "conv2d_10"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_6"
  type: "ReLU"
  bottom: "conv2d_6"
  top: "conv2d_6"
}
layer {
  name: "activation_10"
  type: "ReLU"
  bottom: "conv2d_10"
  top: "conv2d_10"
}
layer {
  name: "concatenate_2"
  type: "Concat"
  bottom: "conv2d_6"
  bottom: "conv2d_10"
  top: "concatenate_2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_11"
  type: "Convolution"
  bottom: "concatenate_2"
  top: "conv2d_11"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_11"
  type: "BatchNorm"
  bottom: "conv2d_11"
  top: "conv2d_11"
}
layer {
  name: "batch_normalization_11s"
  type: "Scale"
  bottom: "conv2d_11"
  top: "conv2d_11"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_11"
  type: "ReLU"
  bottom: "conv2d_11"
  top: "conv2d_11"
}
layer {
  name: "max_pooling2d_2"
  type: "Pooling"
  bottom: "concatenate_2"
  top: "max_pooling2d_2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "concatenate_3"
  type: "Concat"
  bottom: "conv2d_11"
  bottom: "max_pooling2d_2"
  top: "concatenate_3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_15"
  type: "Convolution"
  bottom: "concatenate_3"
  top: "conv2d_15"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_15"
  type: "BatchNorm"
  bottom: "conv2d_15"
  top: "conv2d_15"
}
layer {
  name: "batch_normalization_15s"
  type: "Scale"
  bottom: "conv2d_15"
  top: "conv2d_15"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_15"
  type: "ReLU"
  bottom: "conv2d_15"
  top: "conv2d_15"
}
layer {
  name: "conv2d_13"
  type: "Convolution"
  bottom: "concatenate_3"
  top: "conv2d_13"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_16"
  type: "Convolution"
  bottom: "conv2d_15"
  top: "conv2d_16"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_13"
  type: "BatchNorm"
  bottom: "conv2d_13"
  top: "conv2d_13"
}
layer {
  name: "batch_normalization_13s"
  type: "Scale"
  bottom: "conv2d_13"
  top: "conv2d_13"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_16"
  type: "BatchNorm"
  bottom: "conv2d_16"
  top: "conv2d_16"
}
layer {
  name: "batch_normalization_16s"
  type: "Scale"
  bottom: "conv2d_16"
  top: "conv2d_16"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_13"
  type: "ReLU"
  bottom: "conv2d_13"
  top: "conv2d_13"
}
layer {
  name: "activation_16"
  type: "ReLU"
  bottom: "conv2d_16"
  top: "conv2d_16"
}
layer {
  name: "average_pooling2d_1"
  type: "Pooling"
  bottom: "concatenate_3"
  top: "average_pooling2d_1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_12"
  type: "Convolution"
  bottom: "concatenate_3"
  top: "conv2d_12"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_14"
  type: "Convolution"
  bottom: "conv2d_13"
  top: "conv2d_14"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_17"
  type: "Convolution"
  bottom: "conv2d_16"
  top: "conv2d_17"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_18"
  type: "Convolution"
  bottom: "average_pooling2d_1"
  top: "conv2d_18"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_12"
  type: "BatchNorm"
  bottom: "conv2d_12"
  top: "conv2d_12"
}
layer {
  name: "batch_normalization_12s"
  type: "Scale"
  bottom: "conv2d_12"
  top: "conv2d_12"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_14"
  type: "BatchNorm"
  bottom: "conv2d_14"
  top: "conv2d_14"
}
layer {
  name: "batch_normalization_14s"
  type: "Scale"
  bottom: "conv2d_14"
  top: "conv2d_14"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_17"
  type: "BatchNorm"
  bottom: "conv2d_17"
  top: "conv2d_17"
}
layer {
  name: "batch_normalization_17s"
  type: "Scale"
  bottom: "conv2d_17"
  top: "conv2d_17"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_18"
  type: "BatchNorm"
  bottom: "conv2d_18"
  top: "conv2d_18"
}
layer {
  name: "batch_normalization_18s"
  type: "Scale"
  bottom: "conv2d_18"
  top: "conv2d_18"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_12"
  type: "ReLU"
  bottom: "conv2d_12"
  top: "conv2d_12"
}
layer {
  name: "activation_14"
  type: "ReLU"
  bottom: "conv2d_14"
  top: "conv2d_14"
}
layer {
  name: "activation_17"
  type: "ReLU"
  bottom: "conv2d_17"
  top: "conv2d_17"
}
layer {
  name: "activation_18"
  type: "ReLU"
  bottom: "conv2d_18"
  top: "conv2d_18"
}
layer {
  name: "concatenate_4"
  type: "Concat"
  bottom: "conv2d_12"
  bottom: "conv2d_14"
  bottom: "conv2d_17"
  bottom: "conv2d_18"
  top: "concatenate_4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_22"
  type: "Convolution"
  bottom: "concatenate_4"
  top: "conv2d_22"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_22"
  type: "BatchNorm"
  bottom: "conv2d_22"
  top: "conv2d_22"
}
layer {
  name: "batch_normalization_22s"
  type: "Scale"
  bottom: "conv2d_22"
  top: "conv2d_22"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_22"
  type: "ReLU"
  bottom: "conv2d_22"
  top: "conv2d_22"
}
layer {
  name: "conv2d_20"
  type: "Convolution"
  bottom: "concatenate_4"
  top: "conv2d_20"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_23"
  type: "Convolution"
  bottom: "conv2d_22"
  top: "conv2d_23"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_20"
  type: "BatchNorm"
  bottom: "conv2d_20"
  top: "conv2d_20"
}
layer {
  name: "batch_normalization_20s"
  type: "Scale"
  bottom: "conv2d_20"
  top: "conv2d_20"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_23"
  type: "BatchNorm"
  bottom: "conv2d_23"
  top: "conv2d_23"
}
layer {
  name: "batch_normalization_23s"
  type: "Scale"
  bottom: "conv2d_23"
  top: "conv2d_23"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_20"
  type: "ReLU"
  bottom: "conv2d_20"
  top: "conv2d_20"
}
layer {
  name: "activation_23"
  type: "ReLU"
  bottom: "conv2d_23"
  top: "conv2d_23"
}
layer {
  name: "average_pooling2d_2"
  type: "Pooling"
  bottom: "concatenate_4"
  top: "average_pooling2d_2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_19"
  type: "Convolution"
  bottom: "concatenate_4"
  top: "conv2d_19"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_21"
  type: "Convolution"
  bottom: "conv2d_20"
  top: "conv2d_21"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_24"
  type: "Convolution"
  bottom: "conv2d_23"
  top: "conv2d_24"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_25"
  type: "Convolution"
  bottom: "average_pooling2d_2"
  top: "conv2d_25"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_19"
  type: "BatchNorm"
  bottom: "conv2d_19"
  top: "conv2d_19"
}
layer {
  name: "batch_normalization_19s"
  type: "Scale"
  bottom: "conv2d_19"
  top: "conv2d_19"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_21"
  type: "BatchNorm"
  bottom: "conv2d_21"
  top: "conv2d_21"
}
layer {
  name: "batch_normalization_21s"
  type: "Scale"
  bottom: "conv2d_21"
  top: "conv2d_21"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_24"
  type: "BatchNorm"
  bottom: "conv2d_24"
  top: "conv2d_24"
}
layer {
  name: "batch_normalization_24s"
  type: "Scale"
  bottom: "conv2d_24"
  top: "conv2d_24"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_25"
  type: "BatchNorm"
  bottom: "conv2d_25"
  top: "conv2d_25"
}
layer {
  name: "batch_normalization_25s"
  type: "Scale"
  bottom: "conv2d_25"
  top: "conv2d_25"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_19"
  type: "ReLU"
  bottom: "conv2d_19"
  top: "conv2d_19"
}
layer {
  name: "activation_21"
  type: "ReLU"
  bottom: "conv2d_21"
  top: "conv2d_21"
}
layer {
  name: "activation_24"
  type: "ReLU"
  bottom: "conv2d_24"
  top: "conv2d_24"
}
layer {
  name: "activation_25"
  type: "ReLU"
  bottom: "conv2d_25"
  top: "conv2d_25"
}
layer {
  name: "concatenate_5"
  type: "Concat"
  bottom: "conv2d_19"
  bottom: "conv2d_21"
  bottom: "conv2d_24"
  bottom: "conv2d_25"
  top: "concatenate_5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_29"
  type: "Convolution"
  bottom: "concatenate_5"
  top: "conv2d_29"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_29"
  type: "BatchNorm"
  bottom: "conv2d_29"
  top: "conv2d_29"
}
layer {
  name: "batch_normalization_29s"
  type: "Scale"
  bottom: "conv2d_29"
  top: "conv2d_29"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_29"
  type: "ReLU"
  bottom: "conv2d_29"
  top: "conv2d_29"
}
layer {
  name: "conv2d_27"
  type: "Convolution"
  bottom: "concatenate_5"
  top: "conv2d_27"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_30"
  type: "Convolution"
  bottom: "conv2d_29"
  top: "conv2d_30"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_27"
  type: "BatchNorm"
  bottom: "conv2d_27"
  top: "conv2d_27"
}
layer {
  name: "batch_normalization_27s"
  type: "Scale"
  bottom: "conv2d_27"
  top: "conv2d_27"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_30"
  type: "BatchNorm"
  bottom: "conv2d_30"
  top: "conv2d_30"
}
layer {
  name: "batch_normalization_30s"
  type: "Scale"
  bottom: "conv2d_30"
  top: "conv2d_30"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_27"
  type: "ReLU"
  bottom: "conv2d_27"
  top: "conv2d_27"
}
layer {
  name: "activation_30"
  type: "ReLU"
  bottom: "conv2d_30"
  top: "conv2d_30"
}
layer {
  name: "average_pooling2d_3"
  type: "Pooling"
  bottom: "concatenate_5"
  top: "average_pooling2d_3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_26"
  type: "Convolution"
  bottom: "concatenate_5"
  top: "conv2d_26"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_28"
  type: "Convolution"
  bottom: "conv2d_27"
  top: "conv2d_28"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_31"
  type: "Convolution"
  bottom: "conv2d_30"
  top: "conv2d_31"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_32"
  type: "Convolution"
  bottom: "average_pooling2d_3"
  top: "conv2d_32"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_26"
  type: "BatchNorm"
  bottom: "conv2d_26"
  top: "conv2d_26"
}
layer {
  name: "batch_normalization_26s"
  type: "Scale"
  bottom: "conv2d_26"
  top: "conv2d_26"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_28"
  type: "BatchNorm"
  bottom: "conv2d_28"
  top: "conv2d_28"
}
layer {
  name: "batch_normalization_28s"
  type: "Scale"
  bottom: "conv2d_28"
  top: "conv2d_28"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_31"
  type: "BatchNorm"
  bottom: "conv2d_31"
  top: "conv2d_31"
}
layer {
  name: "batch_normalization_31s"
  type: "Scale"
  bottom: "conv2d_31"
  top: "conv2d_31"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_32"
  type: "BatchNorm"
  bottom: "conv2d_32"
  top: "conv2d_32"
}
layer {
  name: "batch_normalization_32s"
  type: "Scale"
  bottom: "conv2d_32"
  top: "conv2d_32"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_26"
  type: "ReLU"
  bottom: "conv2d_26"
  top: "conv2d_26"
}
layer {
  name: "activation_28"
  type: "ReLU"
  bottom: "conv2d_28"
  top: "conv2d_28"
}
layer {
  name: "activation_31"
  type: "ReLU"
  bottom: "conv2d_31"
  top: "conv2d_31"
}
layer {
  name: "activation_32"
  type: "ReLU"
  bottom: "conv2d_32"
  top: "conv2d_32"
}
layer {
  name: "concatenate_6"
  type: "Concat"
  bottom: "conv2d_26"
  bottom: "conv2d_28"
  bottom: "conv2d_31"
  bottom: "conv2d_32"
  top: "concatenate_6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_36"
  type: "Convolution"
  bottom: "concatenate_6"
  top: "conv2d_36"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_36"
  type: "BatchNorm"
  bottom: "conv2d_36"
  top: "conv2d_36"
}
layer {
  name: "batch_normalization_36s"
  type: "Scale"
  bottom: "conv2d_36"
  top: "conv2d_36"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_36"
  type: "ReLU"
  bottom: "conv2d_36"
  top: "conv2d_36"
}
layer {
  name: "conv2d_34"
  type: "Convolution"
  bottom: "concatenate_6"
  top: "conv2d_34"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_37"
  type: "Convolution"
  bottom: "conv2d_36"
  top: "conv2d_37"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_34"
  type: "BatchNorm"
  bottom: "conv2d_34"
  top: "conv2d_34"
}
layer {
  name: "batch_normalization_34s"
  type: "Scale"
  bottom: "conv2d_34"
  top: "conv2d_34"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_37"
  type: "BatchNorm"
  bottom: "conv2d_37"
  top: "conv2d_37"
}
layer {
  name: "batch_normalization_37s"
  type: "Scale"
  bottom: "conv2d_37"
  top: "conv2d_37"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_34"
  type: "ReLU"
  bottom: "conv2d_34"
  top: "conv2d_34"
}
layer {
  name: "activation_37"
  type: "ReLU"
  bottom: "conv2d_37"
  top: "conv2d_37"
}
layer {
  name: "average_pooling2d_4"
  type: "Pooling"
  bottom: "concatenate_6"
  top: "average_pooling2d_4"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_33"
  type: "Convolution"
  bottom: "concatenate_6"
  top: "conv2d_33"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_35"
  type: "Convolution"
  bottom: "conv2d_34"
  top: "conv2d_35"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_38"
  type: "Convolution"
  bottom: "conv2d_37"
  top: "conv2d_38"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_39"
  type: "Convolution"
  bottom: "average_pooling2d_4"
  top: "conv2d_39"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_33"
  type: "BatchNorm"
  bottom: "conv2d_33"
  top: "conv2d_33"
}
layer {
  name: "batch_normalization_33s"
  type: "Scale"
  bottom: "conv2d_33"
  top: "conv2d_33"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_35"
  type: "BatchNorm"
  bottom: "conv2d_35"
  top: "conv2d_35"
}
layer {
  name: "batch_normalization_35s"
  type: "Scale"
  bottom: "conv2d_35"
  top: "conv2d_35"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_38"
  type: "BatchNorm"
  bottom: "conv2d_38"
  top: "conv2d_38"
}
layer {
  name: "batch_normalization_38s"
  type: "Scale"
  bottom: "conv2d_38"
  top: "conv2d_38"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_39"
  type: "BatchNorm"
  bottom: "conv2d_39"
  top: "conv2d_39"
}
layer {
  name: "batch_normalization_39s"
  type: "Scale"
  bottom: "conv2d_39"
  top: "conv2d_39"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_33"
  type: "ReLU"
  bottom: "conv2d_33"
  top: "conv2d_33"
}
layer {
  name: "activation_35"
  type: "ReLU"
  bottom: "conv2d_35"
  top: "conv2d_35"
}
layer {
  name: "activation_38"
  type: "ReLU"
  bottom: "conv2d_38"
  top: "conv2d_38"
}
layer {
  name: "activation_39"
  type: "ReLU"
  bottom: "conv2d_39"
  top: "conv2d_39"
}
layer {
  name: "concatenate_7"
  type: "Concat"
  bottom: "conv2d_33"
  bottom: "conv2d_35"
  bottom: "conv2d_38"
  bottom: "conv2d_39"
  top: "concatenate_7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_41"
  type: "Convolution"
  bottom: "concatenate_7"
  top: "conv2d_41"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_41"
  type: "BatchNorm"
  bottom: "conv2d_41"
  top: "conv2d_41"
}
layer {
  name: "batch_normalization_41s"
  type: "Scale"
  bottom: "conv2d_41"
  top: "conv2d_41"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_41"
  type: "ReLU"
  bottom: "conv2d_41"
  top: "conv2d_41"
}
layer {
  name: "conv2d_42"
  type: "Convolution"
  bottom: "conv2d_41"
  top: "conv2d_42"
  convolution_param {
    num_output: 224
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_42"
  type: "BatchNorm"
  bottom: "conv2d_42"
  top: "conv2d_42"
}
layer {
  name: "batch_normalization_42s"
  type: "Scale"
  bottom: "conv2d_42"
  top: "conv2d_42"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_42"
  type: "ReLU"
  bottom: "conv2d_42"
  top: "conv2d_42"
}
layer {
  name: "conv2d_40"
  type: "Convolution"
  bottom: "concatenate_7"
  top: "conv2d_40"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_43"
  type: "Convolution"
  bottom: "conv2d_42"
  top: "conv2d_43"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_40"
  type: "BatchNorm"
  bottom: "conv2d_40"
  top: "conv2d_40"
}
layer {
  name: "batch_normalization_40s"
  type: "Scale"
  bottom: "conv2d_40"
  top: "conv2d_40"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_43"
  type: "BatchNorm"
  bottom: "conv2d_43"
  top: "conv2d_43"
}
layer {
  name: "batch_normalization_43s"
  type: "Scale"
  bottom: "conv2d_43"
  top: "conv2d_43"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_40"
  type: "ReLU"
  bottom: "conv2d_40"
  top: "conv2d_40"
}
layer {
  name: "activation_43"
  type: "ReLU"
  bottom: "conv2d_43"
  top: "conv2d_43"
}
layer {
  name: "max_pooling2d_3"
  type: "Pooling"
  bottom: "concatenate_7"
  top: "max_pooling2d_3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "concatenate_8"
  type: "Concat"
  bottom: "conv2d_40"
  bottom: "conv2d_43"
  bottom: "max_pooling2d_3"
  top: "concatenate_8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_48"
  type: "Convolution"
  bottom: "concatenate_8"
  top: "conv2d_48"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_48"
  type: "BatchNorm"
  bottom: "conv2d_48"
  top: "conv2d_48"
}
layer {
  name: "batch_normalization_48s"
  type: "Scale"
  bottom: "conv2d_48"
  top: "conv2d_48"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_48"
  type: "ReLU"
  bottom: "conv2d_48"
  top: "conv2d_48"
}
layer {
  name: "conv2d_49"
  type: "Convolution"
  bottom: "conv2d_48"
  top: "conv2d_49"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_49"
  type: "BatchNorm"
  bottom: "conv2d_49"
  top: "conv2d_49"
}
layer {
  name: "batch_normalization_49s"
  type: "Scale"
  bottom: "conv2d_49"
  top: "conv2d_49"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_49"
  type: "ReLU"
  bottom: "conv2d_49"
  top: "conv2d_49"
}
layer {
  name: "conv2d_45"
  type: "Convolution"
  bottom: "concatenate_8"
  top: "conv2d_45"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_50"
  type: "Convolution"
  bottom: "conv2d_49"
  top: "conv2d_50"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_45"
  type: "BatchNorm"
  bottom: "conv2d_45"
  top: "conv2d_45"
}
layer {
  name: "batch_normalization_45s"
  type: "Scale"
  bottom: "conv2d_45"
  top: "conv2d_45"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_50"
  type: "BatchNorm"
  bottom: "conv2d_50"
  top: "conv2d_50"
}
layer {
  name: "batch_normalization_50s"
  type: "Scale"
  bottom: "conv2d_50"
  top: "conv2d_50"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_45"
  type: "ReLU"
  bottom: "conv2d_45"
  top: "conv2d_45"
}
layer {
  name: "activation_50"
  type: "ReLU"
  bottom: "conv2d_50"
  top: "conv2d_50"
}
layer {
  name: "conv2d_46"
  type: "Convolution"
  bottom: "conv2d_45"
  top: "conv2d_46"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_51"
  type: "Convolution"
  bottom: "conv2d_50"
  top: "conv2d_51"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_46"
  type: "BatchNorm"
  bottom: "conv2d_46"
  top: "conv2d_46"
}
layer {
  name: "batch_normalization_46s"
  type: "Scale"
  bottom: "conv2d_46"
  top: "conv2d_46"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_51"
  type: "BatchNorm"
  bottom: "conv2d_51"
  top: "conv2d_51"
}
layer {
  name: "batch_normalization_51s"
  type: "Scale"
  bottom: "conv2d_51"
  top: "conv2d_51"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_46"
  type: "ReLU"
  bottom: "conv2d_46"
  top: "conv2d_46"
}
layer {
  name: "activation_51"
  type: "ReLU"
  bottom: "conv2d_51"
  top: "conv2d_51"
}
layer {
  name: "average_pooling2d_5"
  type: "Pooling"
  bottom: "concatenate_8"
  top: "average_pooling2d_5"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_44"
  type: "Convolution"
  bottom: "concatenate_8"
  top: "conv2d_44"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_47"
  type: "Convolution"
  bottom: "conv2d_46"
  top: "conv2d_47"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_52"
  type: "Convolution"
  bottom: "conv2d_51"
  top: "conv2d_52"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_53"
  type: "Convolution"
  bottom: "average_pooling2d_5"
  top: "conv2d_53"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_44"
  type: "BatchNorm"
  bottom: "conv2d_44"
  top: "conv2d_44"
}
layer {
  name: "batch_normalization_44s"
  type: "Scale"
  bottom: "conv2d_44"
  top: "conv2d_44"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_47"
  type: "BatchNorm"
  bottom: "conv2d_47"
  top: "conv2d_47"
}
layer {
  name: "batch_normalization_47s"
  type: "Scale"
  bottom: "conv2d_47"
  top: "conv2d_47"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_52"
  type: "BatchNorm"
  bottom: "conv2d_52"
  top: "conv2d_52"
}
layer {
  name: "batch_normalization_52s"
  type: "Scale"
  bottom: "conv2d_52"
  top: "conv2d_52"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_53"
  type: "BatchNorm"
  bottom: "conv2d_53"
  top: "conv2d_53"
}
layer {
  name: "batch_normalization_53s"
  type: "Scale"
  bottom: "conv2d_53"
  top: "conv2d_53"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_44"
  type: "ReLU"
  bottom: "conv2d_44"
  top: "conv2d_44"
}
layer {
  name: "activation_47"
  type: "ReLU"
  bottom: "conv2d_47"
  top: "conv2d_47"
}
layer {
  name: "activation_52"
  type: "ReLU"
  bottom: "conv2d_52"
  top: "conv2d_52"
}
layer {
  name: "activation_53"
  type: "ReLU"
  bottom: "conv2d_53"
  top: "conv2d_53"
}
layer {
  name: "concatenate_9"
  type: "Concat"
  bottom: "conv2d_44"
  bottom: "conv2d_47"
  bottom: "conv2d_52"
  bottom: "conv2d_53"
  top: "concatenate_9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_58"
  type: "Convolution"
  bottom: "concatenate_9"
  top: "conv2d_58"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_58"
  type: "BatchNorm"
  bottom: "conv2d_58"
  top: "conv2d_58"
}
layer {
  name: "batch_normalization_58s"
  type: "Scale"
  bottom: "conv2d_58"
  top: "conv2d_58"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_58"
  type: "ReLU"
  bottom: "conv2d_58"
  top: "conv2d_58"
}
layer {
  name: "conv2d_59"
  type: "Convolution"
  bottom: "conv2d_58"
  top: "conv2d_59"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_59"
  type: "BatchNorm"
  bottom: "conv2d_59"
  top: "conv2d_59"
}
layer {
  name: "batch_normalization_59s"
  type: "Scale"
  bottom: "conv2d_59"
  top: "conv2d_59"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_59"
  type: "ReLU"
  bottom: "conv2d_59"
  top: "conv2d_59"
}
layer {
  name: "conv2d_55"
  type: "Convolution"
  bottom: "concatenate_9"
  top: "conv2d_55"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_60"
  type: "Convolution"
  bottom: "conv2d_59"
  top: "conv2d_60"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_55"
  type: "BatchNorm"
  bottom: "conv2d_55"
  top: "conv2d_55"
}
layer {
  name: "batch_normalization_55s"
  type: "Scale"
  bottom: "conv2d_55"
  top: "conv2d_55"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_60"
  type: "BatchNorm"
  bottom: "conv2d_60"
  top: "conv2d_60"
}
layer {
  name: "batch_normalization_60s"
  type: "Scale"
  bottom: "conv2d_60"
  top: "conv2d_60"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_55"
  type: "ReLU"
  bottom: "conv2d_55"
  top: "conv2d_55"
}
layer {
  name: "activation_60"
  type: "ReLU"
  bottom: "conv2d_60"
  top: "conv2d_60"
}
layer {
  name: "conv2d_56"
  type: "Convolution"
  bottom: "conv2d_55"
  top: "conv2d_56"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_61"
  type: "Convolution"
  bottom: "conv2d_60"
  top: "conv2d_61"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_56"
  type: "BatchNorm"
  bottom: "conv2d_56"
  top: "conv2d_56"
}
layer {
  name: "batch_normalization_56s"
  type: "Scale"
  bottom: "conv2d_56"
  top: "conv2d_56"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_61"
  type: "BatchNorm"
  bottom: "conv2d_61"
  top: "conv2d_61"
}
layer {
  name: "batch_normalization_61s"
  type: "Scale"
  bottom: "conv2d_61"
  top: "conv2d_61"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_56"
  type: "ReLU"
  bottom: "conv2d_56"
  top: "conv2d_56"
}
layer {
  name: "activation_61"
  type: "ReLU"
  bottom: "conv2d_61"
  top: "conv2d_61"
}
layer {
  name: "average_pooling2d_6"
  type: "Pooling"
  bottom: "concatenate_9"
  top: "average_pooling2d_6"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_54"
  type: "Convolution"
  bottom: "concatenate_9"
  top: "conv2d_54"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_57"
  type: "Convolution"
  bottom: "conv2d_56"
  top: "conv2d_57"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_62"
  type: "Convolution"
  bottom: "conv2d_61"
  top: "conv2d_62"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_63"
  type: "Convolution"
  bottom: "average_pooling2d_6"
  top: "conv2d_63"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_54"
  type: "BatchNorm"
  bottom: "conv2d_54"
  top: "conv2d_54"
}
layer {
  name: "batch_normalization_54s"
  type: "Scale"
  bottom: "conv2d_54"
  top: "conv2d_54"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_57"
  type: "BatchNorm"
  bottom: "conv2d_57"
  top: "conv2d_57"
}
layer {
  name: "batch_normalization_57s"
  type: "Scale"
  bottom: "conv2d_57"
  top: "conv2d_57"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_62"
  type: "BatchNorm"
  bottom: "conv2d_62"
  top: "conv2d_62"
}
layer {
  name: "batch_normalization_62s"
  type: "Scale"
  bottom: "conv2d_62"
  top: "conv2d_62"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_63"
  type: "BatchNorm"
  bottom: "conv2d_63"
  top: "conv2d_63"
}
layer {
  name: "batch_normalization_63s"
  type: "Scale"
  bottom: "conv2d_63"
  top: "conv2d_63"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_54"
  type: "ReLU"
  bottom: "conv2d_54"
  top: "conv2d_54"
}
layer {
  name: "activation_57"
  type: "ReLU"
  bottom: "conv2d_57"
  top: "conv2d_57"
}
layer {
  name: "activation_62"
  type: "ReLU"
  bottom: "conv2d_62"
  top: "conv2d_62"
}
layer {
  name: "activation_63"
  type: "ReLU"
  bottom: "conv2d_63"
  top: "conv2d_63"
}
layer {
  name: "concatenate_10"
  type: "Concat"
  bottom: "conv2d_54"
  bottom: "conv2d_57"
  bottom: "conv2d_62"
  bottom: "conv2d_63"
  top: "concatenate_10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_68"
  type: "Convolution"
  bottom: "concatenate_10"
  top: "conv2d_68"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_68"
  type: "BatchNorm"
  bottom: "conv2d_68"
  top: "conv2d_68"
}
layer {
  name: "batch_normalization_68s"
  type: "Scale"
  bottom: "conv2d_68"
  top: "conv2d_68"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_68"
  type: "ReLU"
  bottom: "conv2d_68"
  top: "conv2d_68"
}
layer {
  name: "conv2d_69"
  type: "Convolution"
  bottom: "conv2d_68"
  top: "conv2d_69"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_69"
  type: "BatchNorm"
  bottom: "conv2d_69"
  top: "conv2d_69"
}
layer {
  name: "batch_normalization_69s"
  type: "Scale"
  bottom: "conv2d_69"
  top: "conv2d_69"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_69"
  type: "ReLU"
  bottom: "conv2d_69"
  top: "conv2d_69"
}
layer {
  name: "conv2d_65"
  type: "Convolution"
  bottom: "concatenate_10"
  top: "conv2d_65"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_70"
  type: "Convolution"
  bottom: "conv2d_69"
  top: "conv2d_70"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_65"
  type: "BatchNorm"
  bottom: "conv2d_65"
  top: "conv2d_65"
}
layer {
  name: "batch_normalization_65s"
  type: "Scale"
  bottom: "conv2d_65"
  top: "conv2d_65"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_70"
  type: "BatchNorm"
  bottom: "conv2d_70"
  top: "conv2d_70"
}
layer {
  name: "batch_normalization_70s"
  type: "Scale"
  bottom: "conv2d_70"
  top: "conv2d_70"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_65"
  type: "ReLU"
  bottom: "conv2d_65"
  top: "conv2d_65"
}
layer {
  name: "activation_70"
  type: "ReLU"
  bottom: "conv2d_70"
  top: "conv2d_70"
}
layer {
  name: "conv2d_66"
  type: "Convolution"
  bottom: "conv2d_65"
  top: "conv2d_66"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_71"
  type: "Convolution"
  bottom: "conv2d_70"
  top: "conv2d_71"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_66"
  type: "BatchNorm"
  bottom: "conv2d_66"
  top: "conv2d_66"
}
layer {
  name: "batch_normalization_66s"
  type: "Scale"
  bottom: "conv2d_66"
  top: "conv2d_66"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_71"
  type: "BatchNorm"
  bottom: "conv2d_71"
  top: "conv2d_71"
}
layer {
  name: "batch_normalization_71s"
  type: "Scale"
  bottom: "conv2d_71"
  top: "conv2d_71"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_66"
  type: "ReLU"
  bottom: "conv2d_66"
  top: "conv2d_66"
}
layer {
  name: "activation_71"
  type: "ReLU"
  bottom: "conv2d_71"
  top: "conv2d_71"
}
layer {
  name: "average_pooling2d_7"
  type: "Pooling"
  bottom: "concatenate_10"
  top: "average_pooling2d_7"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_64"
  type: "Convolution"
  bottom: "concatenate_10"
  top: "conv2d_64"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_67"
  type: "Convolution"
  bottom: "conv2d_66"
  top: "conv2d_67"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_72"
  type: "Convolution"
  bottom: "conv2d_71"
  top: "conv2d_72"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_73"
  type: "Convolution"
  bottom: "average_pooling2d_7"
  top: "conv2d_73"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_64"
  type: "BatchNorm"
  bottom: "conv2d_64"
  top: "conv2d_64"
}
layer {
  name: "batch_normalization_64s"
  type: "Scale"
  bottom: "conv2d_64"
  top: "conv2d_64"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_67"
  type: "BatchNorm"
  bottom: "conv2d_67"
  top: "conv2d_67"
}
layer {
  name: "batch_normalization_67s"
  type: "Scale"
  bottom: "conv2d_67"
  top: "conv2d_67"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_72"
  type: "BatchNorm"
  bottom: "conv2d_72"
  top: "conv2d_72"
}
layer {
  name: "batch_normalization_72s"
  type: "Scale"
  bottom: "conv2d_72"
  top: "conv2d_72"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_73"
  type: "BatchNorm"
  bottom: "conv2d_73"
  top: "conv2d_73"
}
layer {
  name: "batch_normalization_73s"
  type: "Scale"
  bottom: "conv2d_73"
  top: "conv2d_73"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_64"
  type: "ReLU"
  bottom: "conv2d_64"
  top: "conv2d_64"
}
layer {
  name: "activation_67"
  type: "ReLU"
  bottom: "conv2d_67"
  top: "conv2d_67"
}
layer {
  name: "activation_72"
  type: "ReLU"
  bottom: "conv2d_72"
  top: "conv2d_72"
}
layer {
  name: "activation_73"
  type: "ReLU"
  bottom: "conv2d_73"
  top: "conv2d_73"
}
layer {
  name: "concatenate_11"
  type: "Concat"
  bottom: "conv2d_64"
  bottom: "conv2d_67"
  bottom: "conv2d_72"
  bottom: "conv2d_73"
  top: "concatenate_11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_78"
  type: "Convolution"
  bottom: "concatenate_11"
  top: "conv2d_78"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_78"
  type: "BatchNorm"
  bottom: "conv2d_78"
  top: "conv2d_78"
}
layer {
  name: "batch_normalization_78s"
  type: "Scale"
  bottom: "conv2d_78"
  top: "conv2d_78"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_78"
  type: "ReLU"
  bottom: "conv2d_78"
  top: "conv2d_78"
}
layer {
  name: "conv2d_79"
  type: "Convolution"
  bottom: "conv2d_78"
  top: "conv2d_79"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_79"
  type: "BatchNorm"
  bottom: "conv2d_79"
  top: "conv2d_79"
}
layer {
  name: "batch_normalization_79s"
  type: "Scale"
  bottom: "conv2d_79"
  top: "conv2d_79"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_79"
  type: "ReLU"
  bottom: "conv2d_79"
  top: "conv2d_79"
}
layer {
  name: "conv2d_75"
  type: "Convolution"
  bottom: "concatenate_11"
  top: "conv2d_75"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_80"
  type: "Convolution"
  bottom: "conv2d_79"
  top: "conv2d_80"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_75"
  type: "BatchNorm"
  bottom: "conv2d_75"
  top: "conv2d_75"
}
layer {
  name: "batch_normalization_75s"
  type: "Scale"
  bottom: "conv2d_75"
  top: "conv2d_75"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_80"
  type: "BatchNorm"
  bottom: "conv2d_80"
  top: "conv2d_80"
}
layer {
  name: "batch_normalization_80s"
  type: "Scale"
  bottom: "conv2d_80"
  top: "conv2d_80"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_75"
  type: "ReLU"
  bottom: "conv2d_75"
  top: "conv2d_75"
}
layer {
  name: "activation_80"
  type: "ReLU"
  bottom: "conv2d_80"
  top: "conv2d_80"
}
layer {
  name: "conv2d_76"
  type: "Convolution"
  bottom: "conv2d_75"
  top: "conv2d_76"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_81"
  type: "Convolution"
  bottom: "conv2d_80"
  top: "conv2d_81"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_76"
  type: "BatchNorm"
  bottom: "conv2d_76"
  top: "conv2d_76"
}
layer {
  name: "batch_normalization_76s"
  type: "Scale"
  bottom: "conv2d_76"
  top: "conv2d_76"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_81"
  type: "BatchNorm"
  bottom: "conv2d_81"
  top: "conv2d_81"
}
layer {
  name: "batch_normalization_81s"
  type: "Scale"
  bottom: "conv2d_81"
  top: "conv2d_81"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_76"
  type: "ReLU"
  bottom: "conv2d_76"
  top: "conv2d_76"
}
layer {
  name: "activation_81"
  type: "ReLU"
  bottom: "conv2d_81"
  top: "conv2d_81"
}
layer {
  name: "average_pooling2d_8"
  type: "Pooling"
  bottom: "concatenate_11"
  top: "average_pooling2d_8"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_74"
  type: "Convolution"
  bottom: "concatenate_11"
  top: "conv2d_74"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_77"
  type: "Convolution"
  bottom: "conv2d_76"
  top: "conv2d_77"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_82"
  type: "Convolution"
  bottom: "conv2d_81"
  top: "conv2d_82"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_83"
  type: "Convolution"
  bottom: "average_pooling2d_8"
  top: "conv2d_83"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_74"
  type: "BatchNorm"
  bottom: "conv2d_74"
  top: "conv2d_74"
}
layer {
  name: "batch_normalization_74s"
  type: "Scale"
  bottom: "conv2d_74"
  top: "conv2d_74"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_77"
  type: "BatchNorm"
  bottom: "conv2d_77"
  top: "conv2d_77"
}
layer {
  name: "batch_normalization_77s"
  type: "Scale"
  bottom: "conv2d_77"
  top: "conv2d_77"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_82"
  type: "BatchNorm"
  bottom: "conv2d_82"
  top: "conv2d_82"
}
layer {
  name: "batch_normalization_82s"
  type: "Scale"
  bottom: "conv2d_82"
  top: "conv2d_82"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_83"
  type: "BatchNorm"
  bottom: "conv2d_83"
  top: "conv2d_83"
}
layer {
  name: "batch_normalization_83s"
  type: "Scale"
  bottom: "conv2d_83"
  top: "conv2d_83"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_74"
  type: "ReLU"
  bottom: "conv2d_74"
  top: "conv2d_74"
}
layer {
  name: "activation_77"
  type: "ReLU"
  bottom: "conv2d_77"
  top: "conv2d_77"
}
layer {
  name: "activation_82"
  type: "ReLU"
  bottom: "conv2d_82"
  top: "conv2d_82"
}
layer {
  name: "activation_83"
  type: "ReLU"
  bottom: "conv2d_83"
  top: "conv2d_83"
}
layer {
  name: "concatenate_12"
  type: "Concat"
  bottom: "conv2d_74"
  bottom: "conv2d_77"
  bottom: "conv2d_82"
  bottom: "conv2d_83"
  top: "concatenate_12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_88"
  type: "Convolution"
  bottom: "concatenate_12"
  top: "conv2d_88"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_88"
  type: "BatchNorm"
  bottom: "conv2d_88"
  top: "conv2d_88"
}
layer {
  name: "batch_normalization_88s"
  type: "Scale"
  bottom: "conv2d_88"
  top: "conv2d_88"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_88"
  type: "ReLU"
  bottom: "conv2d_88"
  top: "conv2d_88"
}
layer {
  name: "conv2d_89"
  type: "Convolution"
  bottom: "conv2d_88"
  top: "conv2d_89"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_89"
  type: "BatchNorm"
  bottom: "conv2d_89"
  top: "conv2d_89"
}
layer {
  name: "batch_normalization_89s"
  type: "Scale"
  bottom: "conv2d_89"
  top: "conv2d_89"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_89"
  type: "ReLU"
  bottom: "conv2d_89"
  top: "conv2d_89"
}
layer {
  name: "conv2d_85"
  type: "Convolution"
  bottom: "concatenate_12"
  top: "conv2d_85"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_90"
  type: "Convolution"
  bottom: "conv2d_89"
  top: "conv2d_90"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_85"
  type: "BatchNorm"
  bottom: "conv2d_85"
  top: "conv2d_85"
}
layer {
  name: "batch_normalization_85s"
  type: "Scale"
  bottom: "conv2d_85"
  top: "conv2d_85"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_90"
  type: "BatchNorm"
  bottom: "conv2d_90"
  top: "conv2d_90"
}
layer {
  name: "batch_normalization_90s"
  type: "Scale"
  bottom: "conv2d_90"
  top: "conv2d_90"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_85"
  type: "ReLU"
  bottom: "conv2d_85"
  top: "conv2d_85"
}
layer {
  name: "activation_90"
  type: "ReLU"
  bottom: "conv2d_90"
  top: "conv2d_90"
}
layer {
  name: "conv2d_86"
  type: "Convolution"
  bottom: "conv2d_85"
  top: "conv2d_86"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_91"
  type: "Convolution"
  bottom: "conv2d_90"
  top: "conv2d_91"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_86"
  type: "BatchNorm"
  bottom: "conv2d_86"
  top: "conv2d_86"
}
layer {
  name: "batch_normalization_86s"
  type: "Scale"
  bottom: "conv2d_86"
  top: "conv2d_86"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_91"
  type: "BatchNorm"
  bottom: "conv2d_91"
  top: "conv2d_91"
}
layer {
  name: "batch_normalization_91s"
  type: "Scale"
  bottom: "conv2d_91"
  top: "conv2d_91"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_86"
  type: "ReLU"
  bottom: "conv2d_86"
  top: "conv2d_86"
}
layer {
  name: "activation_91"
  type: "ReLU"
  bottom: "conv2d_91"
  top: "conv2d_91"
}
layer {
  name: "average_pooling2d_9"
  type: "Pooling"
  bottom: "concatenate_12"
  top: "average_pooling2d_9"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_84"
  type: "Convolution"
  bottom: "concatenate_12"
  top: "conv2d_84"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_87"
  type: "Convolution"
  bottom: "conv2d_86"
  top: "conv2d_87"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_92"
  type: "Convolution"
  bottom: "conv2d_91"
  top: "conv2d_92"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_93"
  type: "Convolution"
  bottom: "average_pooling2d_9"
  top: "conv2d_93"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_84"
  type: "BatchNorm"
  bottom: "conv2d_84"
  top: "conv2d_84"
}
layer {
  name: "batch_normalization_84s"
  type: "Scale"
  bottom: "conv2d_84"
  top: "conv2d_84"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_87"
  type: "BatchNorm"
  bottom: "conv2d_87"
  top: "conv2d_87"
}
layer {
  name: "batch_normalization_87s"
  type: "Scale"
  bottom: "conv2d_87"
  top: "conv2d_87"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_92"
  type: "BatchNorm"
  bottom: "conv2d_92"
  top: "conv2d_92"
}
layer {
  name: "batch_normalization_92s"
  type: "Scale"
  bottom: "conv2d_92"
  top: "conv2d_92"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_93"
  type: "BatchNorm"
  bottom: "conv2d_93"
  top: "conv2d_93"
}
layer {
  name: "batch_normalization_93s"
  type: "Scale"
  bottom: "conv2d_93"
  top: "conv2d_93"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_84"
  type: "ReLU"
  bottom: "conv2d_84"
  top: "conv2d_84"
}
layer {
  name: "activation_87"
  type: "ReLU"
  bottom: "conv2d_87"
  top: "conv2d_87"
}
layer {
  name: "activation_92"
  type: "ReLU"
  bottom: "conv2d_92"
  top: "conv2d_92"
}
layer {
  name: "activation_93"
  type: "ReLU"
  bottom: "conv2d_93"
  top: "conv2d_93"
}
layer {
  name: "concatenate_13"
  type: "Concat"
  bottom: "conv2d_84"
  bottom: "conv2d_87"
  bottom: "conv2d_92"
  bottom: "conv2d_93"
  top: "concatenate_13"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_98"
  type: "Convolution"
  bottom: "concatenate_13"
  top: "conv2d_98"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_98"
  type: "BatchNorm"
  bottom: "conv2d_98"
  top: "conv2d_98"
}
layer {
  name: "batch_normalization_98s"
  type: "Scale"
  bottom: "conv2d_98"
  top: "conv2d_98"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_98"
  type: "ReLU"
  bottom: "conv2d_98"
  top: "conv2d_98"
}
layer {
  name: "conv2d_99"
  type: "Convolution"
  bottom: "conv2d_98"
  top: "conv2d_99"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_99"
  type: "BatchNorm"
  bottom: "conv2d_99"
  top: "conv2d_99"
}
layer {
  name: "batch_normalization_99s"
  type: "Scale"
  bottom: "conv2d_99"
  top: "conv2d_99"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_99"
  type: "ReLU"
  bottom: "conv2d_99"
  top: "conv2d_99"
}
layer {
  name: "conv2d_95"
  type: "Convolution"
  bottom: "concatenate_13"
  top: "conv2d_95"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_100"
  type: "Convolution"
  bottom: "conv2d_99"
  top: "conv2d_100"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_95"
  type: "BatchNorm"
  bottom: "conv2d_95"
  top: "conv2d_95"
}
layer {
  name: "batch_normalization_95s"
  type: "Scale"
  bottom: "conv2d_95"
  top: "conv2d_95"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_100"
  type: "BatchNorm"
  bottom: "conv2d_100"
  top: "conv2d_100"
}
layer {
  name: "batch_normalization_100s"
  type: "Scale"
  bottom: "conv2d_100"
  top: "conv2d_100"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_95"
  type: "ReLU"
  bottom: "conv2d_95"
  top: "conv2d_95"
}
layer {
  name: "activation_100"
  type: "ReLU"
  bottom: "conv2d_100"
  top: "conv2d_100"
}
layer {
  name: "conv2d_96"
  type: "Convolution"
  bottom: "conv2d_95"
  top: "conv2d_96"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_101"
  type: "Convolution"
  bottom: "conv2d_100"
  top: "conv2d_101"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_96"
  type: "BatchNorm"
  bottom: "conv2d_96"
  top: "conv2d_96"
}
layer {
  name: "batch_normalization_96s"
  type: "Scale"
  bottom: "conv2d_96"
  top: "conv2d_96"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_101"
  type: "BatchNorm"
  bottom: "conv2d_101"
  top: "conv2d_101"
}
layer {
  name: "batch_normalization_101s"
  type: "Scale"
  bottom: "conv2d_101"
  top: "conv2d_101"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_96"
  type: "ReLU"
  bottom: "conv2d_96"
  top: "conv2d_96"
}
layer {
  name: "activation_101"
  type: "ReLU"
  bottom: "conv2d_101"
  top: "conv2d_101"
}
layer {
  name: "average_pooling2d_10"
  type: "Pooling"
  bottom: "concatenate_13"
  top: "average_pooling2d_10"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_94"
  type: "Convolution"
  bottom: "concatenate_13"
  top: "conv2d_94"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_97"
  type: "Convolution"
  bottom: "conv2d_96"
  top: "conv2d_97"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_102"
  type: "Convolution"
  bottom: "conv2d_101"
  top: "conv2d_102"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_103"
  type: "Convolution"
  bottom: "average_pooling2d_10"
  top: "conv2d_103"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_94"
  type: "BatchNorm"
  bottom: "conv2d_94"
  top: "conv2d_94"
}
layer {
  name: "batch_normalization_94s"
  type: "Scale"
  bottom: "conv2d_94"
  top: "conv2d_94"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_97"
  type: "BatchNorm"
  bottom: "conv2d_97"
  top: "conv2d_97"
}
layer {
  name: "batch_normalization_97s"
  type: "Scale"
  bottom: "conv2d_97"
  top: "conv2d_97"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_102"
  type: "BatchNorm"
  bottom: "conv2d_102"
  top: "conv2d_102"
}
layer {
  name: "batch_normalization_102s"
  type: "Scale"
  bottom: "conv2d_102"
  top: "conv2d_102"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_103"
  type: "BatchNorm"
  bottom: "conv2d_103"
  top: "conv2d_103"
}
layer {
  name: "batch_normalization_103s"
  type: "Scale"
  bottom: "conv2d_103"
  top: "conv2d_103"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_94"
  type: "ReLU"
  bottom: "conv2d_94"
  top: "conv2d_94"
}
layer {
  name: "activation_97"
  type: "ReLU"
  bottom: "conv2d_97"
  top: "conv2d_97"
}
layer {
  name: "activation_102"
  type: "ReLU"
  bottom: "conv2d_102"
  top: "conv2d_102"
}
layer {
  name: "activation_103"
  type: "ReLU"
  bottom: "conv2d_103"
  top: "conv2d_103"
}
layer {
  name: "concatenate_14"
  type: "Concat"
  bottom: "conv2d_94"
  bottom: "conv2d_97"
  bottom: "conv2d_102"
  bottom: "conv2d_103"
  top: "concatenate_14"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_108"
  type: "Convolution"
  bottom: "concatenate_14"
  top: "conv2d_108"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_108"
  type: "BatchNorm"
  bottom: "conv2d_108"
  top: "conv2d_108"
}
layer {
  name: "batch_normalization_108s"
  type: "Scale"
  bottom: "conv2d_108"
  top: "conv2d_108"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_108"
  type: "ReLU"
  bottom: "conv2d_108"
  top: "conv2d_108"
}
layer {
  name: "conv2d_109"
  type: "Convolution"
  bottom: "conv2d_108"
  top: "conv2d_109"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_109"
  type: "BatchNorm"
  bottom: "conv2d_109"
  top: "conv2d_109"
}
layer {
  name: "batch_normalization_109s"
  type: "Scale"
  bottom: "conv2d_109"
  top: "conv2d_109"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_109"
  type: "ReLU"
  bottom: "conv2d_109"
  top: "conv2d_109"
}
layer {
  name: "conv2d_105"
  type: "Convolution"
  bottom: "concatenate_14"
  top: "conv2d_105"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_110"
  type: "Convolution"
  bottom: "conv2d_109"
  top: "conv2d_110"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_105"
  type: "BatchNorm"
  bottom: "conv2d_105"
  top: "conv2d_105"
}
layer {
  name: "batch_normalization_105s"
  type: "Scale"
  bottom: "conv2d_105"
  top: "conv2d_105"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_110"
  type: "BatchNorm"
  bottom: "conv2d_110"
  top: "conv2d_110"
}
layer {
  name: "batch_normalization_110s"
  type: "Scale"
  bottom: "conv2d_110"
  top: "conv2d_110"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_105"
  type: "ReLU"
  bottom: "conv2d_105"
  top: "conv2d_105"
}
layer {
  name: "activation_110"
  type: "ReLU"
  bottom: "conv2d_110"
  top: "conv2d_110"
}
layer {
  name: "conv2d_106"
  type: "Convolution"
  bottom: "conv2d_105"
  top: "conv2d_106"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_111"
  type: "Convolution"
  bottom: "conv2d_110"
  top: "conv2d_111"
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_106"
  type: "BatchNorm"
  bottom: "conv2d_106"
  top: "conv2d_106"
}
layer {
  name: "batch_normalization_106s"
  type: "Scale"
  bottom: "conv2d_106"
  top: "conv2d_106"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_111"
  type: "BatchNorm"
  bottom: "conv2d_111"
  top: "conv2d_111"
}
layer {
  name: "batch_normalization_111s"
  type: "Scale"
  bottom: "conv2d_111"
  top: "conv2d_111"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_106"
  type: "ReLU"
  bottom: "conv2d_106"
  top: "conv2d_106"
}
layer {
  name: "activation_111"
  type: "ReLU"
  bottom: "conv2d_111"
  top: "conv2d_111"
}
layer {
  name: "average_pooling2d_11"
  type: "Pooling"
  bottom: "concatenate_14"
  top: "average_pooling2d_11"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_104"
  type: "Convolution"
  bottom: "concatenate_14"
  top: "conv2d_104"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_107"
  type: "Convolution"
  bottom: "conv2d_106"
  top: "conv2d_107"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_112"
  type: "Convolution"
  bottom: "conv2d_111"
  top: "conv2d_112"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_113"
  type: "Convolution"
  bottom: "average_pooling2d_11"
  top: "conv2d_113"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_104"
  type: "BatchNorm"
  bottom: "conv2d_104"
  top: "conv2d_104"
}
layer {
  name: "batch_normalization_104s"
  type: "Scale"
  bottom: "conv2d_104"
  top: "conv2d_104"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_107"
  type: "BatchNorm"
  bottom: "conv2d_107"
  top: "conv2d_107"
}
layer {
  name: "batch_normalization_107s"
  type: "Scale"
  bottom: "conv2d_107"
  top: "conv2d_107"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_112"
  type: "BatchNorm"
  bottom: "conv2d_112"
  top: "conv2d_112"
}
layer {
  name: "batch_normalization_112s"
  type: "Scale"
  bottom: "conv2d_112"
  top: "conv2d_112"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_113"
  type: "BatchNorm"
  bottom: "conv2d_113"
  top: "conv2d_113"
}
layer {
  name: "batch_normalization_113s"
  type: "Scale"
  bottom: "conv2d_113"
  top: "conv2d_113"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_104"
  type: "ReLU"
  bottom: "conv2d_104"
  top: "conv2d_104"
}
layer {
  name: "activation_107"
  type: "ReLU"
  bottom: "conv2d_107"
  top: "conv2d_107"
}
layer {
  name: "activation_112"
  type: "ReLU"
  bottom: "conv2d_112"
  top: "conv2d_112"
}
layer {
  name: "activation_113"
  type: "ReLU"
  bottom: "conv2d_113"
  top: "conv2d_113"
}
layer {
  name: "concatenate_15"
  type: "Concat"
  bottom: "conv2d_104"
  bottom: "conv2d_107"
  bottom: "conv2d_112"
  bottom: "conv2d_113"
  top: "concatenate_15"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_116"
  type: "Convolution"
  bottom: "concatenate_15"
  top: "conv2d_116"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_116"
  type: "BatchNorm"
  bottom: "conv2d_116"
  top: "conv2d_116"
}
layer {
  name: "batch_normalization_116s"
  type: "Scale"
  bottom: "conv2d_116"
  top: "conv2d_116"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_116"
  type: "ReLU"
  bottom: "conv2d_116"
  top: "conv2d_116"
}
layer {
  name: "conv2d_117"
  type: "Convolution"
  bottom: "conv2d_116"
  top: "conv2d_117"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_117"
  type: "BatchNorm"
  bottom: "conv2d_117"
  top: "conv2d_117"
}
layer {
  name: "batch_normalization_117s"
  type: "Scale"
  bottom: "conv2d_117"
  top: "conv2d_117"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_117"
  type: "ReLU"
  bottom: "conv2d_117"
  top: "conv2d_117"
}
layer {
  name: "conv2d_114"
  type: "Convolution"
  bottom: "concatenate_15"
  top: "conv2d_114"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_118"
  type: "Convolution"
  bottom: "conv2d_117"
  top: "conv2d_118"
  convolution_param {
    num_output: 320
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_114"
  type: "BatchNorm"
  bottom: "conv2d_114"
  top: "conv2d_114"
}
layer {
  name: "batch_normalization_114s"
  type: "Scale"
  bottom: "conv2d_114"
  top: "conv2d_114"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_118"
  type: "BatchNorm"
  bottom: "conv2d_118"
  top: "conv2d_118"
}
layer {
  name: "batch_normalization_118s"
  type: "Scale"
  bottom: "conv2d_118"
  top: "conv2d_118"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_114"
  type: "ReLU"
  bottom: "conv2d_114"
  top: "conv2d_114"
}
layer {
  name: "activation_118"
  type: "ReLU"
  bottom: "conv2d_118"
  top: "conv2d_118"
}
layer {
  name: "conv2d_115"
  type: "Convolution"
  bottom: "conv2d_114"
  top: "conv2d_115"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_119"
  type: "Convolution"
  bottom: "conv2d_118"
  top: "conv2d_119"
  convolution_param {
    num_output: 320
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_115"
  type: "BatchNorm"
  bottom: "conv2d_115"
  top: "conv2d_115"
}
layer {
  name: "batch_normalization_115s"
  type: "Scale"
  bottom: "conv2d_115"
  top: "conv2d_115"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_119"
  type: "BatchNorm"
  bottom: "conv2d_119"
  top: "conv2d_119"
}
layer {
  name: "batch_normalization_119s"
  type: "Scale"
  bottom: "conv2d_119"
  top: "conv2d_119"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_115"
  type: "ReLU"
  bottom: "conv2d_115"
  top: "conv2d_115"
}
layer {
  name: "activation_119"
  type: "ReLU"
  bottom: "conv2d_119"
  top: "conv2d_119"
}
layer {
  name: "max_pooling2d_4"
  type: "Pooling"
  bottom: "concatenate_15"
  top: "max_pooling2d_4"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "concatenate_16"
  type: "Concat"
  bottom: "conv2d_115"
  bottom: "conv2d_119"
  bottom: "max_pooling2d_4"
  top: "concatenate_16"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_124"
  type: "Convolution"
  bottom: "concatenate_16"
  top: "conv2d_124"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_124"
  type: "BatchNorm"
  bottom: "conv2d_124"
  top: "conv2d_124"
}
layer {
  name: "batch_normalization_124s"
  type: "Scale"
  bottom: "conv2d_124"
  top: "conv2d_124"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_124"
  type: "ReLU"
  bottom: "conv2d_124"
  top: "conv2d_124"
}
layer {
  name: "conv2d_125"
  type: "Convolution"
  bottom: "conv2d_124"
  top: "conv2d_125"
  convolution_param {
    num_output: 448
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_125"
  type: "BatchNorm"
  bottom: "conv2d_125"
  top: "conv2d_125"
}
layer {
  name: "batch_normalization_125s"
  type: "Scale"
  bottom: "conv2d_125"
  top: "conv2d_125"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_125"
  type: "ReLU"
  bottom: "conv2d_125"
  top: "conv2d_125"
}
layer {
  name: "conv2d_121"
  type: "Convolution"
  bottom: "concatenate_16"
  top: "conv2d_121"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_126"
  type: "Convolution"
  bottom: "conv2d_125"
  top: "conv2d_126"
  convolution_param {
    num_output: 512
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "batch_normalization_121"
  type: "BatchNorm"
  bottom: "conv2d_121"
  top: "conv2d_121"
}
layer {
  name: "batch_normalization_121s"
  type: "Scale"
  bottom: "conv2d_121"
  top: "conv2d_121"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_126"
  type: "BatchNorm"
  bottom: "conv2d_126"
  top: "conv2d_126"
}
layer {
  name: "batch_normalization_126s"
  type: "Scale"
  bottom: "conv2d_126"
  top: "conv2d_126"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_121"
  type: "ReLU"
  bottom: "conv2d_121"
  top: "conv2d_121"
}
layer {
  name: "activation_126"
  type: "ReLU"
  bottom: "conv2d_126"
  top: "conv2d_126"
}
layer {
  name: "conv2d_122"
  type: "Convolution"
  bottom: "conv2d_121"
  top: "conv2d_122"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_123"
  type: "Convolution"
  bottom: "conv2d_121"
  top: "conv2d_123"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "conv2d_127"
  type: "Convolution"
  bottom: "conv2d_126"
  top: "conv2d_127"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_128"
  type: "Convolution"
  bottom: "conv2d_126"
  top: "conv2d_128"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "average_pooling2d_12"
  type: "Pooling"
  bottom: "concatenate_16"
  top: "average_pooling2d_12"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_120"
  type: "Convolution"
  bottom: "concatenate_16"
  top: "conv2d_120"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_122"
  type: "BatchNorm"
  bottom: "conv2d_122"
  top: "conv2d_122"
}
layer {
  name: "batch_normalization_122s"
  type: "Scale"
  bottom: "conv2d_122"
  top: "conv2d_122"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_123"
  type: "BatchNorm"
  bottom: "conv2d_123"
  top: "conv2d_123"
}
layer {
  name: "batch_normalization_123s"
  type: "Scale"
  bottom: "conv2d_123"
  top: "conv2d_123"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_127"
  type: "BatchNorm"
  bottom: "conv2d_127"
  top: "conv2d_127"
}
layer {
  name: "batch_normalization_127s"
  type: "Scale"
  bottom: "conv2d_127"
  top: "conv2d_127"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_128"
  type: "BatchNorm"
  bottom: "conv2d_128"
  top: "conv2d_128"
}
layer {
  name: "batch_normalization_128s"
  type: "Scale"
  bottom: "conv2d_128"
  top: "conv2d_128"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2d_129"
  type: "Convolution"
  bottom: "average_pooling2d_12"
  top: "conv2d_129"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_120"
  type: "BatchNorm"
  bottom: "conv2d_120"
  top: "conv2d_120"
}
layer {
  name: "batch_normalization_120s"
  type: "Scale"
  bottom: "conv2d_120"
  top: "conv2d_120"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_122"
  type: "ReLU"
  bottom: "conv2d_122"
  top: "conv2d_122"
}
layer {
  name: "activation_123"
  type: "ReLU"
  bottom: "conv2d_123"
  top: "conv2d_123"
}
layer {
  name: "activation_127"
  type: "ReLU"
  bottom: "conv2d_127"
  top: "conv2d_127"
}
layer {
  name: "activation_128"
  type: "ReLU"
  bottom: "conv2d_128"
  top: "conv2d_128"
}
layer {
  name: "batch_normalization_129"
  type: "BatchNorm"
  bottom: "conv2d_129"
  top: "conv2d_129"
}
layer {
  name: "batch_normalization_129s"
  type: "Scale"
  bottom: "conv2d_129"
  top: "conv2d_129"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_120"
  type: "ReLU"
  bottom: "conv2d_120"
  top: "conv2d_120"
}
layer {
  name: "concatenate_17"
  type: "Concat"
  bottom: "conv2d_122"
  bottom: "conv2d_123"
  top: "concatenate_17"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concatenate_18"
  type: "Concat"
  bottom: "conv2d_127"
  bottom: "conv2d_128"
  top: "concatenate_18"
  concat_param {
    axis: 1
  }
}
layer {
  name: "activation_129"
  type: "ReLU"
  bottom: "conv2d_129"
  top: "conv2d_129"
}
layer {
  name: "concatenate_19"
  type: "Concat"
  bottom: "conv2d_120"
  bottom: "concatenate_17"
  bottom: "concatenate_18"
  bottom: "conv2d_129"
  top: "concatenate_19"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_134"
  type: "Convolution"
  bottom: "concatenate_19"
  top: "conv2d_134"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_134"
  type: "BatchNorm"
  bottom: "conv2d_134"
  top: "conv2d_134"
}
layer {
  name: "batch_normalization_134s"
  type: "Scale"
  bottom: "conv2d_134"
  top: "conv2d_134"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_134"
  type: "ReLU"
  bottom: "conv2d_134"
  top: "conv2d_134"
}
layer {
  name: "conv2d_135"
  type: "Convolution"
  bottom: "conv2d_134"
  top: "conv2d_135"
  convolution_param {
    num_output: 448
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_135"
  type: "BatchNorm"
  bottom: "conv2d_135"
  top: "conv2d_135"
}
layer {
  name: "batch_normalization_135s"
  type: "Scale"
  bottom: "conv2d_135"
  top: "conv2d_135"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_135"
  type: "ReLU"
  bottom: "conv2d_135"
  top: "conv2d_135"
}
layer {
  name: "conv2d_131"
  type: "Convolution"
  bottom: "concatenate_19"
  top: "conv2d_131"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_136"
  type: "Convolution"
  bottom: "conv2d_135"
  top: "conv2d_136"
  convolution_param {
    num_output: 512
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "batch_normalization_131"
  type: "BatchNorm"
  bottom: "conv2d_131"
  top: "conv2d_131"
}
layer {
  name: "batch_normalization_131s"
  type: "Scale"
  bottom: "conv2d_131"
  top: "conv2d_131"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_136"
  type: "BatchNorm"
  bottom: "conv2d_136"
  top: "conv2d_136"
}
layer {
  name: "batch_normalization_136s"
  type: "Scale"
  bottom: "conv2d_136"
  top: "conv2d_136"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_131"
  type: "ReLU"
  bottom: "conv2d_131"
  top: "conv2d_131"
}
layer {
  name: "activation_136"
  type: "ReLU"
  bottom: "conv2d_136"
  top: "conv2d_136"
}
layer {
  name: "conv2d_132"
  type: "Convolution"
  bottom: "conv2d_131"
  top: "conv2d_132"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_133"
  type: "Convolution"
  bottom: "conv2d_131"
  top: "conv2d_133"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "conv2d_137"
  type: "Convolution"
  bottom: "conv2d_136"
  top: "conv2d_137"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_138"
  type: "Convolution"
  bottom: "conv2d_136"
  top: "conv2d_138"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "average_pooling2d_13"
  type: "Pooling"
  bottom: "concatenate_19"
  top: "average_pooling2d_13"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_130"
  type: "Convolution"
  bottom: "concatenate_19"
  top: "conv2d_130"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_132"
  type: "BatchNorm"
  bottom: "conv2d_132"
  top: "conv2d_132"
}
layer {
  name: "batch_normalization_132s"
  type: "Scale"
  bottom: "conv2d_132"
  top: "conv2d_132"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_133"
  type: "BatchNorm"
  bottom: "conv2d_133"
  top: "conv2d_133"
}
layer {
  name: "batch_normalization_133s"
  type: "Scale"
  bottom: "conv2d_133"
  top: "conv2d_133"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_137"
  type: "BatchNorm"
  bottom: "conv2d_137"
  top: "conv2d_137"
}
layer {
  name: "batch_normalization_137s"
  type: "Scale"
  bottom: "conv2d_137"
  top: "conv2d_137"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_138"
  type: "BatchNorm"
  bottom: "conv2d_138"
  top: "conv2d_138"
}
layer {
  name: "batch_normalization_138s"
  type: "Scale"
  bottom: "conv2d_138"
  top: "conv2d_138"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2d_139"
  type: "Convolution"
  bottom: "average_pooling2d_13"
  top: "conv2d_139"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_130"
  type: "BatchNorm"
  bottom: "conv2d_130"
  top: "conv2d_130"
}
layer {
  name: "batch_normalization_130s"
  type: "Scale"
  bottom: "conv2d_130"
  top: "conv2d_130"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_132"
  type: "ReLU"
  bottom: "conv2d_132"
  top: "conv2d_132"
}
layer {
  name: "activation_133"
  type: "ReLU"
  bottom: "conv2d_133"
  top: "conv2d_133"
}
layer {
  name: "activation_137"
  type: "ReLU"
  bottom: "conv2d_137"
  top: "conv2d_137"
}
layer {
  name: "activation_138"
  type: "ReLU"
  bottom: "conv2d_138"
  top: "conv2d_138"
}
layer {
  name: "batch_normalization_139"
  type: "BatchNorm"
  bottom: "conv2d_139"
  top: "conv2d_139"
}
layer {
  name: "batch_normalization_139s"
  type: "Scale"
  bottom: "conv2d_139"
  top: "conv2d_139"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_130"
  type: "ReLU"
  bottom: "conv2d_130"
  top: "conv2d_130"
}
layer {
  name: "concatenate_20"
  type: "Concat"
  bottom: "conv2d_132"
  bottom: "conv2d_133"
  top: "concatenate_20"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concatenate_21"
  type: "Concat"
  bottom: "conv2d_137"
  bottom: "conv2d_138"
  top: "concatenate_21"
  concat_param {
    axis: 1
  }
}
layer {
  name: "activation_139"
  type: "ReLU"
  bottom: "conv2d_139"
  top: "conv2d_139"
}
layer {
  name: "concatenate_22"
  type: "Concat"
  bottom: "conv2d_130"
  bottom: "concatenate_20"
  bottom: "concatenate_21"
  bottom: "conv2d_139"
  top: "concatenate_22"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_144"
  type: "Convolution"
  bottom: "concatenate_22"
  top: "conv2d_144"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_144"
  type: "BatchNorm"
  bottom: "conv2d_144"
  top: "conv2d_144"
}
layer {
  name: "batch_normalization_144s"
  type: "Scale"
  bottom: "conv2d_144"
  top: "conv2d_144"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_144"
  type: "ReLU"
  bottom: "conv2d_144"
  top: "conv2d_144"
}
layer {
  name: "conv2d_145"
  type: "Convolution"
  bottom: "conv2d_144"
  top: "conv2d_145"
  convolution_param {
    num_output: 448
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_145"
  type: "BatchNorm"
  bottom: "conv2d_145"
  top: "conv2d_145"
}
layer {
  name: "batch_normalization_145s"
  type: "Scale"
  bottom: "conv2d_145"
  top: "conv2d_145"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_145"
  type: "ReLU"
  bottom: "conv2d_145"
  top: "conv2d_145"
}
layer {
  name: "conv2d_141"
  type: "Convolution"
  bottom: "concatenate_22"
  top: "conv2d_141"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_146"
  type: "Convolution"
  bottom: "conv2d_145"
  top: "conv2d_146"
  convolution_param {
    num_output: 512
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "batch_normalization_141"
  type: "BatchNorm"
  bottom: "conv2d_141"
  top: "conv2d_141"
}
layer {
  name: "batch_normalization_141s"
  type: "Scale"
  bottom: "conv2d_141"
  top: "conv2d_141"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_146"
  type: "BatchNorm"
  bottom: "conv2d_146"
  top: "conv2d_146"
}
layer {
  name: "batch_normalization_146s"
  type: "Scale"
  bottom: "conv2d_146"
  top: "conv2d_146"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_141"
  type: "ReLU"
  bottom: "conv2d_141"
  top: "conv2d_141"
}
layer {
  name: "activation_146"
  type: "ReLU"
  bottom: "conv2d_146"
  top: "conv2d_146"
}
layer {
  name: "conv2d_142"
  type: "Convolution"
  bottom: "conv2d_141"
  top: "conv2d_142"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_143"
  type: "Convolution"
  bottom: "conv2d_141"
  top: "conv2d_143"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "conv2d_147"
  type: "Convolution"
  bottom: "conv2d_146"
  top: "conv2d_147"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_148"
  type: "Convolution"
  bottom: "conv2d_146"
  top: "conv2d_148"
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "average_pooling2d_14"
  type: "Pooling"
  bottom: "concatenate_22"
  top: "average_pooling2d_14"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_140"
  type: "Convolution"
  bottom: "concatenate_22"
  top: "conv2d_140"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_142"
  type: "BatchNorm"
  bottom: "conv2d_142"
  top: "conv2d_142"
}
layer {
  name: "batch_normalization_142s"
  type: "Scale"
  bottom: "conv2d_142"
  top: "conv2d_142"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_143"
  type: "BatchNorm"
  bottom: "conv2d_143"
  top: "conv2d_143"
}
layer {
  name: "batch_normalization_143s"
  type: "Scale"
  bottom: "conv2d_143"
  top: "conv2d_143"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_147"
  type: "BatchNorm"
  bottom: "conv2d_147"
  top: "conv2d_147"
}
layer {
  name: "batch_normalization_147s"
  type: "Scale"
  bottom: "conv2d_147"
  top: "conv2d_147"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_148"
  type: "BatchNorm"
  bottom: "conv2d_148"
  top: "conv2d_148"
}
layer {
  name: "batch_normalization_148s"
  type: "Scale"
  bottom: "conv2d_148"
  top: "conv2d_148"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2d_149"
  type: "Convolution"
  bottom: "average_pooling2d_14"
  top: "conv2d_149"
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_140"
  type: "BatchNorm"
  bottom: "conv2d_140"
  top: "conv2d_140"
}
layer {
  name: "batch_normalization_140s"
  type: "Scale"
  bottom: "conv2d_140"
  top: "conv2d_140"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_142"
  type: "ReLU"
  bottom: "conv2d_142"
  top: "conv2d_142"
}
layer {
  name: "activation_143"
  type: "ReLU"
  bottom: "conv2d_143"
  top: "conv2d_143"
}
layer {
  name: "activation_147"
  type: "ReLU"
  bottom: "conv2d_147"
  top: "conv2d_147"
}
layer {
  name: "activation_148"
  type: "ReLU"
  bottom: "conv2d_148"
  top: "conv2d_148"
}
layer {
  name: "batch_normalization_149"
  type: "BatchNorm"
  bottom: "conv2d_149"
  top: "conv2d_149"
}
layer {
  name: "batch_normalization_149s"
  type: "Scale"
  bottom: "conv2d_149"
  top: "conv2d_149"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_140"
  type: "ReLU"
  bottom: "conv2d_140"
  top: "conv2d_140"
}
layer {
  name: "concatenate_23"
  type: "Concat"
  bottom: "conv2d_142"
  bottom: "conv2d_143"
  top: "concatenate_23"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concatenate_24"
  type: "Concat"
  bottom: "conv2d_147"
  bottom: "conv2d_148"
  top: "concatenate_24"
  concat_param {
    axis: 1
  }
}
layer {
  name: "activation_149"
  type: "ReLU"
  bottom: "conv2d_149"
  top: "conv2d_149"
}
layer {
  name: "concatenate_25"
  type: "Concat"
  bottom: "conv2d_140"
  bottom: "concatenate_23"
  bottom: "concatenate_24"
  bottom: "conv2d_149"
  top: "concatenate_25"
  concat_param {
    axis: 1
  }
}
layer {
  name: "average_pooling2d_15"
  type: "Pooling"
  bottom: "concatenate_25"
  top: "average_pooling2d_15"
  pooling_param {
    pool: AVE
    kernel_size: 8
    stride: 8
  }
}
layer {
  name: "dropout_1"
  type: "Dropout"
  bottom: "average_pooling2d_15"
  top: "dropout_1"
  dropout_param {
    dropout_ratio: 0.800000011921
  }
}
layer {
  name: "flatten_1"
  type: "Flatten"
  bottom: "dropout_1"
  top: "flatten_1"
}
layer {
  name: "dense_1"
  type: "InnerProduct"
  bottom: "flatten_1"
  top: "dense_1"
  inner_product_param {
    num_output: 1001
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "dense_1s"
  type: "Softmax"
  bottom: "dense_1"
  top: "dense_1"
}
