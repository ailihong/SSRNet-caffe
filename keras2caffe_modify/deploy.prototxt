input: "data"
input_dim: 1
input_dim: 3
input_dim: 299
input_dim: 299
layer {
  name: "conv2d_1"
  type: "Convolution"
  bottom: "data"
  top: "conv2d_1"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_1"
  type: "BatchNorm"
  bottom: "conv2d_1"
  top: "conv2d_1"
}
layer {
  name: "batch_normalization_1s"
  type: "Scale"
  bottom: "conv2d_1"
  top: "conv2d_1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_1"
  type: "ReLU"
  bottom: "conv2d_1"
  top: "conv2d_1"
}
layer {
  name: "conv2d_2"
  type: "Convolution"
  bottom: "conv2d_1"
  top: "conv2d_2"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_2"
  type: "BatchNorm"
  bottom: "conv2d_2"
  top: "conv2d_2"
}
layer {
  name: "batch_normalization_2s"
  type: "Scale"
  bottom: "conv2d_2"
  top: "conv2d_2"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_2"
  type: "ReLU"
  bottom: "conv2d_2"
  top: "conv2d_2"
}
layer {
  name: "conv2d_3"
  type: "Convolution"
  bottom: "conv2d_2"
  top: "conv2d_3"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_3"
  type: "BatchNorm"
  bottom: "conv2d_3"
  top: "conv2d_3"
}
layer {
  name: "batch_normalization_3s"
  type: "Scale"
  bottom: "conv2d_3"
  top: "conv2d_3"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_3"
  type: "ReLU"
  bottom: "conv2d_3"
  top: "conv2d_3"
}
layer {
  name: "max_pooling2d_1"
  type: "Pooling"
  bottom: "conv2d_3"
  top: "max_pooling2d_1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_4"
  type: "Convolution"
  bottom: "max_pooling2d_1"
  top: "conv2d_4"
  convolution_param {
    num_output: 80
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_4"
  type: "BatchNorm"
  bottom: "conv2d_4"
  top: "conv2d_4"
}
layer {
  name: "batch_normalization_4s"
  type: "Scale"
  bottom: "conv2d_4"
  top: "conv2d_4"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_4"
  type: "ReLU"
  bottom: "conv2d_4"
  top: "conv2d_4"
}
layer {
  name: "conv2d_5"
  type: "Convolution"
  bottom: "conv2d_4"
  top: "conv2d_5"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_5"
  type: "BatchNorm"
  bottom: "conv2d_5"
  top: "conv2d_5"
}
layer {
  name: "batch_normalization_5s"
  type: "Scale"
  bottom: "conv2d_5"
  top: "conv2d_5"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_5"
  type: "ReLU"
  bottom: "conv2d_5"
  top: "conv2d_5"
}
layer {
  name: "max_pooling2d_2"
  type: "Pooling"
  bottom: "conv2d_5"
  top: "max_pooling2d_2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_9"
  type: "Convolution"
  bottom: "max_pooling2d_2"
  top: "conv2d_9"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_9"
  type: "BatchNorm"
  bottom: "conv2d_9"
  top: "conv2d_9"
}
layer {
  name: "batch_normalization_9s"
  type: "Scale"
  bottom: "conv2d_9"
  top: "conv2d_9"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_9"
  type: "ReLU"
  bottom: "conv2d_9"
  top: "conv2d_9"
}
layer {
  name: "conv2d_7"
  type: "Convolution"
  bottom: "max_pooling2d_2"
  top: "conv2d_7"
  convolution_param {
    num_output: 48
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_10"
  type: "Convolution"
  bottom: "conv2d_9"
  top: "conv2d_10"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_7"
  type: "BatchNorm"
  bottom: "conv2d_7"
  top: "conv2d_7"
}
layer {
  name: "batch_normalization_7s"
  type: "Scale"
  bottom: "conv2d_7"
  top: "conv2d_7"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_10"
  type: "BatchNorm"
  bottom: "conv2d_10"
  top: "conv2d_10"
}
layer {
  name: "batch_normalization_10s"
  type: "Scale"
  bottom: "conv2d_10"
  top: "conv2d_10"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_7"
  type: "ReLU"
  bottom: "conv2d_7"
  top: "conv2d_7"
}
layer {
  name: "activation_10"
  type: "ReLU"
  bottom: "conv2d_10"
  top: "conv2d_10"
}
layer {
  name: "average_pooling2d_1"
  type: "Pooling"
  bottom: "max_pooling2d_2"
  top: "average_pooling2d_1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_6"
  type: "Convolution"
  bottom: "max_pooling2d_2"
  top: "conv2d_6"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_8"
  type: "Convolution"
  bottom: "conv2d_7"
  top: "conv2d_8"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "conv2d_11"
  type: "Convolution"
  bottom: "conv2d_10"
  top: "conv2d_11"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_12"
  type: "Convolution"
  bottom: "average_pooling2d_1"
  top: "conv2d_12"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_6"
  type: "BatchNorm"
  bottom: "conv2d_6"
  top: "conv2d_6"
}
layer {
  name: "batch_normalization_6s"
  type: "Scale"
  bottom: "conv2d_6"
  top: "conv2d_6"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_8"
  type: "BatchNorm"
  bottom: "conv2d_8"
  top: "conv2d_8"
}
layer {
  name: "batch_normalization_8s"
  type: "Scale"
  bottom: "conv2d_8"
  top: "conv2d_8"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_11"
  type: "BatchNorm"
  bottom: "conv2d_11"
  top: "conv2d_11"
}
layer {
  name: "batch_normalization_11s"
  type: "Scale"
  bottom: "conv2d_11"
  top: "conv2d_11"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_12"
  type: "BatchNorm"
  bottom: "conv2d_12"
  top: "conv2d_12"
}
layer {
  name: "batch_normalization_12s"
  type: "Scale"
  bottom: "conv2d_12"
  top: "conv2d_12"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_6"
  type: "ReLU"
  bottom: "conv2d_6"
  top: "conv2d_6"
}
layer {
  name: "activation_8"
  type: "ReLU"
  bottom: "conv2d_8"
  top: "conv2d_8"
}
layer {
  name: "activation_11"
  type: "ReLU"
  bottom: "conv2d_11"
  top: "conv2d_11"
}
layer {
  name: "activation_12"
  type: "ReLU"
  bottom: "conv2d_12"
  top: "conv2d_12"
}
layer {
  name: "mixed0"
  type: "Concat"
  bottom: "conv2d_6"
  bottom: "conv2d_8"
  bottom: "conv2d_11"
  bottom: "conv2d_12"
  top: "mixed0"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_16"
  type: "Convolution"
  bottom: "mixed0"
  top: "conv2d_16"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_16"
  type: "BatchNorm"
  bottom: "conv2d_16"
  top: "conv2d_16"
}
layer {
  name: "batch_normalization_16s"
  type: "Scale"
  bottom: "conv2d_16"
  top: "conv2d_16"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_16"
  type: "ReLU"
  bottom: "conv2d_16"
  top: "conv2d_16"
}
layer {
  name: "conv2d_14"
  type: "Convolution"
  bottom: "mixed0"
  top: "conv2d_14"
  convolution_param {
    num_output: 48
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_17"
  type: "Convolution"
  bottom: "conv2d_16"
  top: "conv2d_17"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_14"
  type: "BatchNorm"
  bottom: "conv2d_14"
  top: "conv2d_14"
}
layer {
  name: "batch_normalization_14s"
  type: "Scale"
  bottom: "conv2d_14"
  top: "conv2d_14"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_17"
  type: "BatchNorm"
  bottom: "conv2d_17"
  top: "conv2d_17"
}
layer {
  name: "batch_normalization_17s"
  type: "Scale"
  bottom: "conv2d_17"
  top: "conv2d_17"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_14"
  type: "ReLU"
  bottom: "conv2d_14"
  top: "conv2d_14"
}
layer {
  name: "activation_17"
  type: "ReLU"
  bottom: "conv2d_17"
  top: "conv2d_17"
}
layer {
  name: "average_pooling2d_2"
  type: "Pooling"
  bottom: "mixed0"
  top: "average_pooling2d_2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_13"
  type: "Convolution"
  bottom: "mixed0"
  top: "conv2d_13"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_15"
  type: "Convolution"
  bottom: "conv2d_14"
  top: "conv2d_15"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "conv2d_18"
  type: "Convolution"
  bottom: "conv2d_17"
  top: "conv2d_18"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_19"
  type: "Convolution"
  bottom: "average_pooling2d_2"
  top: "conv2d_19"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_13"
  type: "BatchNorm"
  bottom: "conv2d_13"
  top: "conv2d_13"
}
layer {
  name: "batch_normalization_13s"
  type: "Scale"
  bottom: "conv2d_13"
  top: "conv2d_13"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_15"
  type: "BatchNorm"
  bottom: "conv2d_15"
  top: "conv2d_15"
}
layer {
  name: "batch_normalization_15s"
  type: "Scale"
  bottom: "conv2d_15"
  top: "conv2d_15"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_18"
  type: "BatchNorm"
  bottom: "conv2d_18"
  top: "conv2d_18"
}
layer {
  name: "batch_normalization_18s"
  type: "Scale"
  bottom: "conv2d_18"
  top: "conv2d_18"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_19"
  type: "BatchNorm"
  bottom: "conv2d_19"
  top: "conv2d_19"
}
layer {
  name: "batch_normalization_19s"
  type: "Scale"
  bottom: "conv2d_19"
  top: "conv2d_19"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_13"
  type: "ReLU"
  bottom: "conv2d_13"
  top: "conv2d_13"
}
layer {
  name: "activation_15"
  type: "ReLU"
  bottom: "conv2d_15"
  top: "conv2d_15"
}
layer {
  name: "activation_18"
  type: "ReLU"
  bottom: "conv2d_18"
  top: "conv2d_18"
}
layer {
  name: "activation_19"
  type: "ReLU"
  bottom: "conv2d_19"
  top: "conv2d_19"
}
layer {
  name: "mixed1"
  type: "Concat"
  bottom: "conv2d_13"
  bottom: "conv2d_15"
  bottom: "conv2d_18"
  bottom: "conv2d_19"
  top: "mixed1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_23"
  type: "Convolution"
  bottom: "mixed1"
  top: "conv2d_23"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_23"
  type: "BatchNorm"
  bottom: "conv2d_23"
  top: "conv2d_23"
}
layer {
  name: "batch_normalization_23s"
  type: "Scale"
  bottom: "conv2d_23"
  top: "conv2d_23"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_23"
  type: "ReLU"
  bottom: "conv2d_23"
  top: "conv2d_23"
}
layer {
  name: "conv2d_21"
  type: "Convolution"
  bottom: "mixed1"
  top: "conv2d_21"
  convolution_param {
    num_output: 48
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_24"
  type: "Convolution"
  bottom: "conv2d_23"
  top: "conv2d_24"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_21"
  type: "BatchNorm"
  bottom: "conv2d_21"
  top: "conv2d_21"
}
layer {
  name: "batch_normalization_21s"
  type: "Scale"
  bottom: "conv2d_21"
  top: "conv2d_21"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_24"
  type: "BatchNorm"
  bottom: "conv2d_24"
  top: "conv2d_24"
}
layer {
  name: "batch_normalization_24s"
  type: "Scale"
  bottom: "conv2d_24"
  top: "conv2d_24"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_21"
  type: "ReLU"
  bottom: "conv2d_21"
  top: "conv2d_21"
}
layer {
  name: "activation_24"
  type: "ReLU"
  bottom: "conv2d_24"
  top: "conv2d_24"
}
layer {
  name: "average_pooling2d_3"
  type: "Pooling"
  bottom: "mixed1"
  top: "average_pooling2d_3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_20"
  type: "Convolution"
  bottom: "mixed1"
  top: "conv2d_20"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_22"
  type: "Convolution"
  bottom: "conv2d_21"
  top: "conv2d_22"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "conv2d_25"
  type: "Convolution"
  bottom: "conv2d_24"
  top: "conv2d_25"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "conv2d_26"
  type: "Convolution"
  bottom: "average_pooling2d_3"
  top: "conv2d_26"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_20"
  type: "BatchNorm"
  bottom: "conv2d_20"
  top: "conv2d_20"
}
layer {
  name: "batch_normalization_20s"
  type: "Scale"
  bottom: "conv2d_20"
  top: "conv2d_20"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_22"
  type: "BatchNorm"
  bottom: "conv2d_22"
  top: "conv2d_22"
}
layer {
  name: "batch_normalization_22s"
  type: "Scale"
  bottom: "conv2d_22"
  top: "conv2d_22"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_25"
  type: "BatchNorm"
  bottom: "conv2d_25"
  top: "conv2d_25"
}
layer {
  name: "batch_normalization_25s"
  type: "Scale"
  bottom: "conv2d_25"
  top: "conv2d_25"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_26"
  type: "BatchNorm"
  bottom: "conv2d_26"
  top: "conv2d_26"
}
layer {
  name: "batch_normalization_26s"
  type: "Scale"
  bottom: "conv2d_26"
  top: "conv2d_26"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_20"
  type: "ReLU"
  bottom: "conv2d_20"
  top: "conv2d_20"
}
layer {
  name: "activation_22"
  type: "ReLU"
  bottom: "conv2d_22"
  top: "conv2d_22"
}
layer {
  name: "activation_25"
  type: "ReLU"
  bottom: "conv2d_25"
  top: "conv2d_25"
}
layer {
  name: "activation_26"
  type: "ReLU"
  bottom: "conv2d_26"
  top: "conv2d_26"
}
layer {
  name: "mixed2"
  type: "Concat"
  bottom: "conv2d_20"
  bottom: "conv2d_22"
  bottom: "conv2d_25"
  bottom: "conv2d_26"
  top: "mixed2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_28"
  type: "Convolution"
  bottom: "mixed2"
  top: "conv2d_28"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_28"
  type: "BatchNorm"
  bottom: "conv2d_28"
  top: "conv2d_28"
}
layer {
  name: "batch_normalization_28s"
  type: "Scale"
  bottom: "conv2d_28"
  top: "conv2d_28"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_28"
  type: "ReLU"
  bottom: "conv2d_28"
  top: "conv2d_28"
}
layer {
  name: "conv2d_29"
  type: "Convolution"
  bottom: "conv2d_28"
  top: "conv2d_29"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_29"
  type: "BatchNorm"
  bottom: "conv2d_29"
  top: "conv2d_29"
}
layer {
  name: "batch_normalization_29s"
  type: "Scale"
  bottom: "conv2d_29"
  top: "conv2d_29"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_29"
  type: "ReLU"
  bottom: "conv2d_29"
  top: "conv2d_29"
}
layer {
  name: "conv2d_27"
  type: "Convolution"
  bottom: "mixed2"
  top: "conv2d_27"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_30"
  type: "Convolution"
  bottom: "conv2d_29"
  top: "conv2d_30"
  convolution_param {
    num_output: 96
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_27"
  type: "BatchNorm"
  bottom: "conv2d_27"
  top: "conv2d_27"
}
layer {
  name: "batch_normalization_27s"
  type: "Scale"
  bottom: "conv2d_27"
  top: "conv2d_27"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_30"
  type: "BatchNorm"
  bottom: "conv2d_30"
  top: "conv2d_30"
}
layer {
  name: "batch_normalization_30s"
  type: "Scale"
  bottom: "conv2d_30"
  top: "conv2d_30"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_27"
  type: "ReLU"
  bottom: "conv2d_27"
  top: "conv2d_27"
}
layer {
  name: "activation_30"
  type: "ReLU"
  bottom: "conv2d_30"
  top: "conv2d_30"
}
layer {
  name: "max_pooling2d_3"
  type: "Pooling"
  bottom: "mixed2"
  top: "max_pooling2d_3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "mixed3"
  type: "Concat"
  bottom: "conv2d_27"
  bottom: "conv2d_30"
  bottom: "max_pooling2d_3"
  top: "mixed3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_35"
  type: "Convolution"
  bottom: "mixed3"
  top: "conv2d_35"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_35"
  type: "BatchNorm"
  bottom: "conv2d_35"
  top: "conv2d_35"
}
layer {
  name: "batch_normalization_35s"
  type: "Scale"
  bottom: "conv2d_35"
  top: "conv2d_35"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_35"
  type: "ReLU"
  bottom: "conv2d_35"
  top: "conv2d_35"
}
layer {
  name: "conv2d_36"
  type: "Convolution"
  bottom: "conv2d_35"
  top: "conv2d_36"
  convolution_param {
    num_output: 128
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_36"
  type: "BatchNorm"
  bottom: "conv2d_36"
  top: "conv2d_36"
}
layer {
  name: "batch_normalization_36s"
  type: "Scale"
  bottom: "conv2d_36"
  top: "conv2d_36"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_36"
  type: "ReLU"
  bottom: "conv2d_36"
  top: "conv2d_36"
}
layer {
  name: "conv2d_32"
  type: "Convolution"
  bottom: "mixed3"
  top: "conv2d_32"
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_37"
  type: "Convolution"
  bottom: "conv2d_36"
  top: "conv2d_37"
  convolution_param {
    num_output: 128
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_32"
  type: "BatchNorm"
  bottom: "conv2d_32"
  top: "conv2d_32"
}
layer {
  name: "batch_normalization_32s"
  type: "Scale"
  bottom: "conv2d_32"
  top: "conv2d_32"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_37"
  type: "BatchNorm"
  bottom: "conv2d_37"
  top: "conv2d_37"
}
layer {
  name: "batch_normalization_37s"
  type: "Scale"
  bottom: "conv2d_37"
  top: "conv2d_37"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_32"
  type: "ReLU"
  bottom: "conv2d_32"
  top: "conv2d_32"
}
layer {
  name: "activation_37"
  type: "ReLU"
  bottom: "conv2d_37"
  top: "conv2d_37"
}
layer {
  name: "conv2d_33"
  type: "Convolution"
  bottom: "conv2d_32"
  top: "conv2d_33"
  convolution_param {
    num_output: 128
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_38"
  type: "Convolution"
  bottom: "conv2d_37"
  top: "conv2d_38"
  convolution_param {
    num_output: 128
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_33"
  type: "BatchNorm"
  bottom: "conv2d_33"
  top: "conv2d_33"
}
layer {
  name: "batch_normalization_33s"
  type: "Scale"
  bottom: "conv2d_33"
  top: "conv2d_33"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_38"
  type: "BatchNorm"
  bottom: "conv2d_38"
  top: "conv2d_38"
}
layer {
  name: "batch_normalization_38s"
  type: "Scale"
  bottom: "conv2d_38"
  top: "conv2d_38"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_33"
  type: "ReLU"
  bottom: "conv2d_33"
  top: "conv2d_33"
}
layer {
  name: "activation_38"
  type: "ReLU"
  bottom: "conv2d_38"
  top: "conv2d_38"
}
layer {
  name: "average_pooling2d_4"
  type: "Pooling"
  bottom: "mixed3"
  top: "average_pooling2d_4"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_31"
  type: "Convolution"
  bottom: "mixed3"
  top: "conv2d_31"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_34"
  type: "Convolution"
  bottom: "conv2d_33"
  top: "conv2d_34"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_39"
  type: "Convolution"
  bottom: "conv2d_38"
  top: "conv2d_39"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_40"
  type: "Convolution"
  bottom: "average_pooling2d_4"
  top: "conv2d_40"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_31"
  type: "BatchNorm"
  bottom: "conv2d_31"
  top: "conv2d_31"
}
layer {
  name: "batch_normalization_31s"
  type: "Scale"
  bottom: "conv2d_31"
  top: "conv2d_31"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_34"
  type: "BatchNorm"
  bottom: "conv2d_34"
  top: "conv2d_34"
}
layer {
  name: "batch_normalization_34s"
  type: "Scale"
  bottom: "conv2d_34"
  top: "conv2d_34"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_39"
  type: "BatchNorm"
  bottom: "conv2d_39"
  top: "conv2d_39"
}
layer {
  name: "batch_normalization_39s"
  type: "Scale"
  bottom: "conv2d_39"
  top: "conv2d_39"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_40"
  type: "BatchNorm"
  bottom: "conv2d_40"
  top: "conv2d_40"
}
layer {
  name: "batch_normalization_40s"
  type: "Scale"
  bottom: "conv2d_40"
  top: "conv2d_40"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_31"
  type: "ReLU"
  bottom: "conv2d_31"
  top: "conv2d_31"
}
layer {
  name: "activation_34"
  type: "ReLU"
  bottom: "conv2d_34"
  top: "conv2d_34"
}
layer {
  name: "activation_39"
  type: "ReLU"
  bottom: "conv2d_39"
  top: "conv2d_39"
}
layer {
  name: "activation_40"
  type: "ReLU"
  bottom: "conv2d_40"
  top: "conv2d_40"
}
layer {
  name: "mixed4"
  type: "Concat"
  bottom: "conv2d_31"
  bottom: "conv2d_34"
  bottom: "conv2d_39"
  bottom: "conv2d_40"
  top: "mixed4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_45"
  type: "Convolution"
  bottom: "mixed4"
  top: "conv2d_45"
  convolution_param {
    num_output: 160
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_45"
  type: "BatchNorm"
  bottom: "conv2d_45"
  top: "conv2d_45"
}
layer {
  name: "batch_normalization_45s"
  type: "Scale"
  bottom: "conv2d_45"
  top: "conv2d_45"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_45"
  type: "ReLU"
  bottom: "conv2d_45"
  top: "conv2d_45"
}
layer {
  name: "conv2d_46"
  type: "Convolution"
  bottom: "conv2d_45"
  top: "conv2d_46"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_46"
  type: "BatchNorm"
  bottom: "conv2d_46"
  top: "conv2d_46"
}
layer {
  name: "batch_normalization_46s"
  type: "Scale"
  bottom: "conv2d_46"
  top: "conv2d_46"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_46"
  type: "ReLU"
  bottom: "conv2d_46"
  top: "conv2d_46"
}
layer {
  name: "conv2d_42"
  type: "Convolution"
  bottom: "mixed4"
  top: "conv2d_42"
  convolution_param {
    num_output: 160
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_47"
  type: "Convolution"
  bottom: "conv2d_46"
  top: "conv2d_47"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_42"
  type: "BatchNorm"
  bottom: "conv2d_42"
  top: "conv2d_42"
}
layer {
  name: "batch_normalization_42s"
  type: "Scale"
  bottom: "conv2d_42"
  top: "conv2d_42"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_47"
  type: "BatchNorm"
  bottom: "conv2d_47"
  top: "conv2d_47"
}
layer {
  name: "batch_normalization_47s"
  type: "Scale"
  bottom: "conv2d_47"
  top: "conv2d_47"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_42"
  type: "ReLU"
  bottom: "conv2d_42"
  top: "conv2d_42"
}
layer {
  name: "activation_47"
  type: "ReLU"
  bottom: "conv2d_47"
  top: "conv2d_47"
}
layer {
  name: "conv2d_43"
  type: "Convolution"
  bottom: "conv2d_42"
  top: "conv2d_43"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_48"
  type: "Convolution"
  bottom: "conv2d_47"
  top: "conv2d_48"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_43"
  type: "BatchNorm"
  bottom: "conv2d_43"
  top: "conv2d_43"
}
layer {
  name: "batch_normalization_43s"
  type: "Scale"
  bottom: "conv2d_43"
  top: "conv2d_43"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_48"
  type: "BatchNorm"
  bottom: "conv2d_48"
  top: "conv2d_48"
}
layer {
  name: "batch_normalization_48s"
  type: "Scale"
  bottom: "conv2d_48"
  top: "conv2d_48"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_43"
  type: "ReLU"
  bottom: "conv2d_43"
  top: "conv2d_43"
}
layer {
  name: "activation_48"
  type: "ReLU"
  bottom: "conv2d_48"
  top: "conv2d_48"
}
layer {
  name: "average_pooling2d_5"
  type: "Pooling"
  bottom: "mixed4"
  top: "average_pooling2d_5"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_41"
  type: "Convolution"
  bottom: "mixed4"
  top: "conv2d_41"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_44"
  type: "Convolution"
  bottom: "conv2d_43"
  top: "conv2d_44"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_49"
  type: "Convolution"
  bottom: "conv2d_48"
  top: "conv2d_49"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_50"
  type: "Convolution"
  bottom: "average_pooling2d_5"
  top: "conv2d_50"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_41"
  type: "BatchNorm"
  bottom: "conv2d_41"
  top: "conv2d_41"
}
layer {
  name: "batch_normalization_41s"
  type: "Scale"
  bottom: "conv2d_41"
  top: "conv2d_41"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_44"
  type: "BatchNorm"
  bottom: "conv2d_44"
  top: "conv2d_44"
}
layer {
  name: "batch_normalization_44s"
  type: "Scale"
  bottom: "conv2d_44"
  top: "conv2d_44"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_49"
  type: "BatchNorm"
  bottom: "conv2d_49"
  top: "conv2d_49"
}
layer {
  name: "batch_normalization_49s"
  type: "Scale"
  bottom: "conv2d_49"
  top: "conv2d_49"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_50"
  type: "BatchNorm"
  bottom: "conv2d_50"
  top: "conv2d_50"
}
layer {
  name: "batch_normalization_50s"
  type: "Scale"
  bottom: "conv2d_50"
  top: "conv2d_50"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_41"
  type: "ReLU"
  bottom: "conv2d_41"
  top: "conv2d_41"
}
layer {
  name: "activation_44"
  type: "ReLU"
  bottom: "conv2d_44"
  top: "conv2d_44"
}
layer {
  name: "activation_49"
  type: "ReLU"
  bottom: "conv2d_49"
  top: "conv2d_49"
}
layer {
  name: "activation_50"
  type: "ReLU"
  bottom: "conv2d_50"
  top: "conv2d_50"
}
layer {
  name: "mixed5"
  type: "Concat"
  bottom: "conv2d_41"
  bottom: "conv2d_44"
  bottom: "conv2d_49"
  bottom: "conv2d_50"
  top: "mixed5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_55"
  type: "Convolution"
  bottom: "mixed5"
  top: "conv2d_55"
  convolution_param {
    num_output: 160
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_55"
  type: "BatchNorm"
  bottom: "conv2d_55"
  top: "conv2d_55"
}
layer {
  name: "batch_normalization_55s"
  type: "Scale"
  bottom: "conv2d_55"
  top: "conv2d_55"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_55"
  type: "ReLU"
  bottom: "conv2d_55"
  top: "conv2d_55"
}
layer {
  name: "conv2d_56"
  type: "Convolution"
  bottom: "conv2d_55"
  top: "conv2d_56"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_56"
  type: "BatchNorm"
  bottom: "conv2d_56"
  top: "conv2d_56"
}
layer {
  name: "batch_normalization_56s"
  type: "Scale"
  bottom: "conv2d_56"
  top: "conv2d_56"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_56"
  type: "ReLU"
  bottom: "conv2d_56"
  top: "conv2d_56"
}
layer {
  name: "conv2d_52"
  type: "Convolution"
  bottom: "mixed5"
  top: "conv2d_52"
  convolution_param {
    num_output: 160
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_57"
  type: "Convolution"
  bottom: "conv2d_56"
  top: "conv2d_57"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_52"
  type: "BatchNorm"
  bottom: "conv2d_52"
  top: "conv2d_52"
}
layer {
  name: "batch_normalization_52s"
  type: "Scale"
  bottom: "conv2d_52"
  top: "conv2d_52"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_57"
  type: "BatchNorm"
  bottom: "conv2d_57"
  top: "conv2d_57"
}
layer {
  name: "batch_normalization_57s"
  type: "Scale"
  bottom: "conv2d_57"
  top: "conv2d_57"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_52"
  type: "ReLU"
  bottom: "conv2d_52"
  top: "conv2d_52"
}
layer {
  name: "activation_57"
  type: "ReLU"
  bottom: "conv2d_57"
  top: "conv2d_57"
}
layer {
  name: "conv2d_53"
  type: "Convolution"
  bottom: "conv2d_52"
  top: "conv2d_53"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_58"
  type: "Convolution"
  bottom: "conv2d_57"
  top: "conv2d_58"
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_53"
  type: "BatchNorm"
  bottom: "conv2d_53"
  top: "conv2d_53"
}
layer {
  name: "batch_normalization_53s"
  type: "Scale"
  bottom: "conv2d_53"
  top: "conv2d_53"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_58"
  type: "BatchNorm"
  bottom: "conv2d_58"
  top: "conv2d_58"
}
layer {
  name: "batch_normalization_58s"
  type: "Scale"
  bottom: "conv2d_58"
  top: "conv2d_58"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_53"
  type: "ReLU"
  bottom: "conv2d_53"
  top: "conv2d_53"
}
layer {
  name: "activation_58"
  type: "ReLU"
  bottom: "conv2d_58"
  top: "conv2d_58"
}
layer {
  name: "average_pooling2d_6"
  type: "Pooling"
  bottom: "mixed5"
  top: "average_pooling2d_6"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_51"
  type: "Convolution"
  bottom: "mixed5"
  top: "conv2d_51"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_54"
  type: "Convolution"
  bottom: "conv2d_53"
  top: "conv2d_54"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_59"
  type: "Convolution"
  bottom: "conv2d_58"
  top: "conv2d_59"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_60"
  type: "Convolution"
  bottom: "average_pooling2d_6"
  top: "conv2d_60"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_51"
  type: "BatchNorm"
  bottom: "conv2d_51"
  top: "conv2d_51"
}
layer {
  name: "batch_normalization_51s"
  type: "Scale"
  bottom: "conv2d_51"
  top: "conv2d_51"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_54"
  type: "BatchNorm"
  bottom: "conv2d_54"
  top: "conv2d_54"
}
layer {
  name: "batch_normalization_54s"
  type: "Scale"
  bottom: "conv2d_54"
  top: "conv2d_54"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_59"
  type: "BatchNorm"
  bottom: "conv2d_59"
  top: "conv2d_59"
}
layer {
  name: "batch_normalization_59s"
  type: "Scale"
  bottom: "conv2d_59"
  top: "conv2d_59"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_60"
  type: "BatchNorm"
  bottom: "conv2d_60"
  top: "conv2d_60"
}
layer {
  name: "batch_normalization_60s"
  type: "Scale"
  bottom: "conv2d_60"
  top: "conv2d_60"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_51"
  type: "ReLU"
  bottom: "conv2d_51"
  top: "conv2d_51"
}
layer {
  name: "activation_54"
  type: "ReLU"
  bottom: "conv2d_54"
  top: "conv2d_54"
}
layer {
  name: "activation_59"
  type: "ReLU"
  bottom: "conv2d_59"
  top: "conv2d_59"
}
layer {
  name: "activation_60"
  type: "ReLU"
  bottom: "conv2d_60"
  top: "conv2d_60"
}
layer {
  name: "mixed6"
  type: "Concat"
  bottom: "conv2d_51"
  bottom: "conv2d_54"
  bottom: "conv2d_59"
  bottom: "conv2d_60"
  top: "mixed6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_65"
  type: "Convolution"
  bottom: "mixed6"
  top: "conv2d_65"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_65"
  type: "BatchNorm"
  bottom: "conv2d_65"
  top: "conv2d_65"
}
layer {
  name: "batch_normalization_65s"
  type: "Scale"
  bottom: "conv2d_65"
  top: "conv2d_65"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_65"
  type: "ReLU"
  bottom: "conv2d_65"
  top: "conv2d_65"
}
layer {
  name: "conv2d_66"
  type: "Convolution"
  bottom: "conv2d_65"
  top: "conv2d_66"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_66"
  type: "BatchNorm"
  bottom: "conv2d_66"
  top: "conv2d_66"
}
layer {
  name: "batch_normalization_66s"
  type: "Scale"
  bottom: "conv2d_66"
  top: "conv2d_66"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_66"
  type: "ReLU"
  bottom: "conv2d_66"
  top: "conv2d_66"
}
layer {
  name: "conv2d_62"
  type: "Convolution"
  bottom: "mixed6"
  top: "conv2d_62"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_67"
  type: "Convolution"
  bottom: "conv2d_66"
  top: "conv2d_67"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_62"
  type: "BatchNorm"
  bottom: "conv2d_62"
  top: "conv2d_62"
}
layer {
  name: "batch_normalization_62s"
  type: "Scale"
  bottom: "conv2d_62"
  top: "conv2d_62"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_67"
  type: "BatchNorm"
  bottom: "conv2d_67"
  top: "conv2d_67"
}
layer {
  name: "batch_normalization_67s"
  type: "Scale"
  bottom: "conv2d_67"
  top: "conv2d_67"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_62"
  type: "ReLU"
  bottom: "conv2d_62"
  top: "conv2d_62"
}
layer {
  name: "activation_67"
  type: "ReLU"
  bottom: "conv2d_67"
  top: "conv2d_67"
}
layer {
  name: "conv2d_63"
  type: "Convolution"
  bottom: "conv2d_62"
  top: "conv2d_63"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_68"
  type: "Convolution"
  bottom: "conv2d_67"
  top: "conv2d_68"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_63"
  type: "BatchNorm"
  bottom: "conv2d_63"
  top: "conv2d_63"
}
layer {
  name: "batch_normalization_63s"
  type: "Scale"
  bottom: "conv2d_63"
  top: "conv2d_63"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_68"
  type: "BatchNorm"
  bottom: "conv2d_68"
  top: "conv2d_68"
}
layer {
  name: "batch_normalization_68s"
  type: "Scale"
  bottom: "conv2d_68"
  top: "conv2d_68"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_63"
  type: "ReLU"
  bottom: "conv2d_63"
  top: "conv2d_63"
}
layer {
  name: "activation_68"
  type: "ReLU"
  bottom: "conv2d_68"
  top: "conv2d_68"
}
layer {
  name: "average_pooling2d_7"
  type: "Pooling"
  bottom: "mixed6"
  top: "average_pooling2d_7"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_61"
  type: "Convolution"
  bottom: "mixed6"
  top: "conv2d_61"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_64"
  type: "Convolution"
  bottom: "conv2d_63"
  top: "conv2d_64"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "conv2d_69"
  type: "Convolution"
  bottom: "conv2d_68"
  top: "conv2d_69"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "conv2d_70"
  type: "Convolution"
  bottom: "average_pooling2d_7"
  top: "conv2d_70"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_61"
  type: "BatchNorm"
  bottom: "conv2d_61"
  top: "conv2d_61"
}
layer {
  name: "batch_normalization_61s"
  type: "Scale"
  bottom: "conv2d_61"
  top: "conv2d_61"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_64"
  type: "BatchNorm"
  bottom: "conv2d_64"
  top: "conv2d_64"
}
layer {
  name: "batch_normalization_64s"
  type: "Scale"
  bottom: "conv2d_64"
  top: "conv2d_64"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_69"
  type: "BatchNorm"
  bottom: "conv2d_69"
  top: "conv2d_69"
}
layer {
  name: "batch_normalization_69s"
  type: "Scale"
  bottom: "conv2d_69"
  top: "conv2d_69"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_70"
  type: "BatchNorm"
  bottom: "conv2d_70"
  top: "conv2d_70"
}
layer {
  name: "batch_normalization_70s"
  type: "Scale"
  bottom: "conv2d_70"
  top: "conv2d_70"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_61"
  type: "ReLU"
  bottom: "conv2d_61"
  top: "conv2d_61"
}
layer {
  name: "activation_64"
  type: "ReLU"
  bottom: "conv2d_64"
  top: "conv2d_64"
}
layer {
  name: "activation_69"
  type: "ReLU"
  bottom: "conv2d_69"
  top: "conv2d_69"
}
layer {
  name: "activation_70"
  type: "ReLU"
  bottom: "conv2d_70"
  top: "conv2d_70"
}
layer {
  name: "mixed7"
  type: "Concat"
  bottom: "conv2d_61"
  bottom: "conv2d_64"
  bottom: "conv2d_69"
  bottom: "conv2d_70"
  top: "mixed7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_73"
  type: "Convolution"
  bottom: "mixed7"
  top: "conv2d_73"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_73"
  type: "BatchNorm"
  bottom: "conv2d_73"
  top: "conv2d_73"
}
layer {
  name: "batch_normalization_73s"
  type: "Scale"
  bottom: "conv2d_73"
  top: "conv2d_73"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_73"
  type: "ReLU"
  bottom: "conv2d_73"
  top: "conv2d_73"
}
layer {
  name: "conv2d_74"
  type: "Convolution"
  bottom: "conv2d_73"
  top: "conv2d_74"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "batch_normalization_74"
  type: "BatchNorm"
  bottom: "conv2d_74"
  top: "conv2d_74"
}
layer {
  name: "batch_normalization_74s"
  type: "Scale"
  bottom: "conv2d_74"
  top: "conv2d_74"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_74"
  type: "ReLU"
  bottom: "conv2d_74"
  top: "conv2d_74"
}
layer {
  name: "conv2d_71"
  type: "Convolution"
  bottom: "mixed7"
  top: "conv2d_71"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_75"
  type: "Convolution"
  bottom: "conv2d_74"
  top: "conv2d_75"
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "batch_normalization_71"
  type: "BatchNorm"
  bottom: "conv2d_71"
  top: "conv2d_71"
}
layer {
  name: "batch_normalization_71s"
  type: "Scale"
  bottom: "conv2d_71"
  top: "conv2d_71"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_75"
  type: "BatchNorm"
  bottom: "conv2d_75"
  top: "conv2d_75"
}
layer {
  name: "batch_normalization_75s"
  type: "Scale"
  bottom: "conv2d_75"
  top: "conv2d_75"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_71"
  type: "ReLU"
  bottom: "conv2d_71"
  top: "conv2d_71"
}
layer {
  name: "activation_75"
  type: "ReLU"
  bottom: "conv2d_75"
  top: "conv2d_75"
}
layer {
  name: "conv2d_72"
  type: "Convolution"
  bottom: "conv2d_71"
  top: "conv2d_72"
  convolution_param {
    num_output: 320
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2d_76"
  type: "Convolution"
  bottom: "conv2d_75"
  top: "conv2d_76"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "batch_normalization_72"
  type: "BatchNorm"
  bottom: "conv2d_72"
  top: "conv2d_72"
}
layer {
  name: "batch_normalization_72s"
  type: "Scale"
  bottom: "conv2d_72"
  top: "conv2d_72"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_76"
  type: "BatchNorm"
  bottom: "conv2d_76"
  top: "conv2d_76"
}
layer {
  name: "batch_normalization_76s"
  type: "Scale"
  bottom: "conv2d_76"
  top: "conv2d_76"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_72"
  type: "ReLU"
  bottom: "conv2d_72"
  top: "conv2d_72"
}
layer {
  name: "activation_76"
  type: "ReLU"
  bottom: "conv2d_76"
  top: "conv2d_76"
}
layer {
  name: "max_pooling2d_4"
  type: "Pooling"
  bottom: "mixed7"
  top: "max_pooling2d_4"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "mixed8"
  type: "Concat"
  bottom: "conv2d_72"
  bottom: "conv2d_76"
  bottom: "max_pooling2d_4"
  top: "mixed8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_81"
  type: "Convolution"
  bottom: "mixed8"
  top: "conv2d_81"
  convolution_param {
    num_output: 448
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_81"
  type: "BatchNorm"
  bottom: "conv2d_81"
  top: "conv2d_81"
}
layer {
  name: "batch_normalization_81s"
  type: "Scale"
  bottom: "conv2d_81"
  top: "conv2d_81"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_81"
  type: "ReLU"
  bottom: "conv2d_81"
  top: "conv2d_81"
}
layer {
  name: "conv2d_78"
  type: "Convolution"
  bottom: "mixed8"
  top: "conv2d_78"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_82"
  type: "Convolution"
  bottom: "conv2d_81"
  top: "conv2d_82"
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_78"
  type: "BatchNorm"
  bottom: "conv2d_78"
  top: "conv2d_78"
}
layer {
  name: "batch_normalization_78s"
  type: "Scale"
  bottom: "conv2d_78"
  top: "conv2d_78"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_82"
  type: "BatchNorm"
  bottom: "conv2d_82"
  top: "conv2d_82"
}
layer {
  name: "batch_normalization_82s"
  type: "Scale"
  bottom: "conv2d_82"
  top: "conv2d_82"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_78"
  type: "ReLU"
  bottom: "conv2d_78"
  top: "conv2d_78"
}
layer {
  name: "activation_82"
  type: "ReLU"
  bottom: "conv2d_82"
  top: "conv2d_82"
}
layer {
  name: "conv2d_79"
  type: "Convolution"
  bottom: "conv2d_78"
  top: "conv2d_79"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_80"
  type: "Convolution"
  bottom: "conv2d_78"
  top: "conv2d_80"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "conv2d_83"
  type: "Convolution"
  bottom: "conv2d_82"
  top: "conv2d_83"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_84"
  type: "Convolution"
  bottom: "conv2d_82"
  top: "conv2d_84"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "average_pooling2d_8"
  type: "Pooling"
  bottom: "mixed8"
  top: "average_pooling2d_8"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_77"
  type: "Convolution"
  bottom: "mixed8"
  top: "conv2d_77"
  convolution_param {
    num_output: 320
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_79"
  type: "BatchNorm"
  bottom: "conv2d_79"
  top: "conv2d_79"
}
layer {
  name: "batch_normalization_79s"
  type: "Scale"
  bottom: "conv2d_79"
  top: "conv2d_79"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_80"
  type: "BatchNorm"
  bottom: "conv2d_80"
  top: "conv2d_80"
}
layer {
  name: "batch_normalization_80s"
  type: "Scale"
  bottom: "conv2d_80"
  top: "conv2d_80"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_83"
  type: "BatchNorm"
  bottom: "conv2d_83"
  top: "conv2d_83"
}
layer {
  name: "batch_normalization_83s"
  type: "Scale"
  bottom: "conv2d_83"
  top: "conv2d_83"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_84"
  type: "BatchNorm"
  bottom: "conv2d_84"
  top: "conv2d_84"
}
layer {
  name: "batch_normalization_84s"
  type: "Scale"
  bottom: "conv2d_84"
  top: "conv2d_84"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2d_85"
  type: "Convolution"
  bottom: "average_pooling2d_8"
  top: "conv2d_85"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_77"
  type: "BatchNorm"
  bottom: "conv2d_77"
  top: "conv2d_77"
}
layer {
  name: "batch_normalization_77s"
  type: "Scale"
  bottom: "conv2d_77"
  top: "conv2d_77"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_79"
  type: "ReLU"
  bottom: "conv2d_79"
  top: "conv2d_79"
}
layer {
  name: "activation_80"
  type: "ReLU"
  bottom: "conv2d_80"
  top: "conv2d_80"
}
layer {
  name: "activation_83"
  type: "ReLU"
  bottom: "conv2d_83"
  top: "conv2d_83"
}
layer {
  name: "activation_84"
  type: "ReLU"
  bottom: "conv2d_84"
  top: "conv2d_84"
}
layer {
  name: "batch_normalization_85"
  type: "BatchNorm"
  bottom: "conv2d_85"
  top: "conv2d_85"
}
layer {
  name: "batch_normalization_85s"
  type: "Scale"
  bottom: "conv2d_85"
  top: "conv2d_85"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_77"
  type: "ReLU"
  bottom: "conv2d_77"
  top: "conv2d_77"
}
layer {
  name: "mixed9_0"
  type: "Concat"
  bottom: "conv2d_79"
  bottom: "conv2d_80"
  top: "mixed9_0"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concatenate_1"
  type: "Concat"
  bottom: "conv2d_83"
  bottom: "conv2d_84"
  top: "concatenate_1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "activation_85"
  type: "ReLU"
  bottom: "conv2d_85"
  top: "conv2d_85"
}
layer {
  name: "mixed9"
  type: "Concat"
  bottom: "conv2d_77"
  bottom: "mixed9_0"
  bottom: "concatenate_1"
  bottom: "conv2d_85"
  top: "mixed9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "conv2d_90"
  type: "Convolution"
  bottom: "mixed9"
  top: "conv2d_90"
  convolution_param {
    num_output: 448
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_90"
  type: "BatchNorm"
  bottom: "conv2d_90"
  top: "conv2d_90"
}
layer {
  name: "batch_normalization_90s"
  type: "Scale"
  bottom: "conv2d_90"
  top: "conv2d_90"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_90"
  type: "ReLU"
  bottom: "conv2d_90"
  top: "conv2d_90"
}
layer {
  name: "conv2d_87"
  type: "Convolution"
  bottom: "mixed9"
  top: "conv2d_87"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "conv2d_91"
  type: "Convolution"
  bottom: "conv2d_90"
  top: "conv2d_91"
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "batch_normalization_87"
  type: "BatchNorm"
  bottom: "conv2d_87"
  top: "conv2d_87"
}
layer {
  name: "batch_normalization_87s"
  type: "Scale"
  bottom: "conv2d_87"
  top: "conv2d_87"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_91"
  type: "BatchNorm"
  bottom: "conv2d_91"
  top: "conv2d_91"
}
layer {
  name: "batch_normalization_91s"
  type: "Scale"
  bottom: "conv2d_91"
  top: "conv2d_91"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_87"
  type: "ReLU"
  bottom: "conv2d_87"
  top: "conv2d_87"
}
layer {
  name: "activation_91"
  type: "ReLU"
  bottom: "conv2d_91"
  top: "conv2d_91"
}
layer {
  name: "conv2d_88"
  type: "Convolution"
  bottom: "conv2d_87"
  top: "conv2d_88"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_89"
  type: "Convolution"
  bottom: "conv2d_87"
  top: "conv2d_89"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "conv2d_92"
  type: "Convolution"
  bottom: "conv2d_91"
  top: "conv2d_92"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "conv2d_93"
  type: "Convolution"
  bottom: "conv2d_91"
  top: "conv2d_93"
  convolution_param {
    num_output: 384
    bias_term: false
    stride: 1
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "average_pooling2d_9"
  type: "Pooling"
  bottom: "mixed9"
  top: "average_pooling2d_9"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "conv2d_86"
  type: "Convolution"
  bottom: "mixed9"
  top: "conv2d_86"
  convolution_param {
    num_output: 320
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_88"
  type: "BatchNorm"
  bottom: "conv2d_88"
  top: "conv2d_88"
}
layer {
  name: "batch_normalization_88s"
  type: "Scale"
  bottom: "conv2d_88"
  top: "conv2d_88"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_89"
  type: "BatchNorm"
  bottom: "conv2d_89"
  top: "conv2d_89"
}
layer {
  name: "batch_normalization_89s"
  type: "Scale"
  bottom: "conv2d_89"
  top: "conv2d_89"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_92"
  type: "BatchNorm"
  bottom: "conv2d_92"
  top: "conv2d_92"
}
layer {
  name: "batch_normalization_92s"
  type: "Scale"
  bottom: "conv2d_92"
  top: "conv2d_92"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "batch_normalization_93"
  type: "BatchNorm"
  bottom: "conv2d_93"
  top: "conv2d_93"
}
layer {
  name: "batch_normalization_93s"
  type: "Scale"
  bottom: "conv2d_93"
  top: "conv2d_93"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2d_94"
  type: "Convolution"
  bottom: "average_pooling2d_9"
  top: "conv2d_94"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "batch_normalization_86"
  type: "BatchNorm"
  bottom: "conv2d_86"
  top: "conv2d_86"
}
layer {
  name: "batch_normalization_86s"
  type: "Scale"
  bottom: "conv2d_86"
  top: "conv2d_86"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_88"
  type: "ReLU"
  bottom: "conv2d_88"
  top: "conv2d_88"
}
layer {
  name: "activation_89"
  type: "ReLU"
  bottom: "conv2d_89"
  top: "conv2d_89"
}
layer {
  name: "activation_92"
  type: "ReLU"
  bottom: "conv2d_92"
  top: "conv2d_92"
}
layer {
  name: "activation_93"
  type: "ReLU"
  bottom: "conv2d_93"
  top: "conv2d_93"
}
layer {
  name: "batch_normalization_94"
  type: "BatchNorm"
  bottom: "conv2d_94"
  top: "conv2d_94"
}
layer {
  name: "batch_normalization_94s"
  type: "Scale"
  bottom: "conv2d_94"
  top: "conv2d_94"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_86"
  type: "ReLU"
  bottom: "conv2d_86"
  top: "conv2d_86"
}
layer {
  name: "mixed9_1"
  type: "Concat"
  bottom: "conv2d_88"
  bottom: "conv2d_89"
  top: "mixed9_1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concatenate_2"
  type: "Concat"
  bottom: "conv2d_92"
  bottom: "conv2d_93"
  top: "concatenate_2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "activation_94"
  type: "ReLU"
  bottom: "conv2d_94"
  top: "conv2d_94"
}
layer {
  name: "mixed10"
  type: "Concat"
  bottom: "conv2d_86"
  bottom: "mixed9_1"
  bottom: "concatenate_2"
  bottom: "conv2d_94"
  top: "mixed10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "avg_pool"
  type: "Pooling"
  bottom: "mixed10"
  top: "avg_pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "predictions"
  type: "InnerProduct"
  bottom: "avg_pool"
  top: "predictions"
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "predictionss"
  type: "Softmax"
  bottom: "predictions"
  top: "predictions"
}
